Unsupervised elicitation
and Learning the prior
Martín Soto, June 25
TL;DR: Anthropic’s recent Unsupervised elicitation empirical paper is deeply related to past conceptual work on Learning the prior. Applying this past framing reveals interesting empirical follow-ups!

Sketching the connection
In Unsupervised elicitation, Jiaxin et al propose an algorithm (Internal Coherence Maximization) to elicit capabilities in classification tasks from base models without human examples. Instead of extracting some human knowledge (for example, which assistant answers are most helpful) from supervised examples, they extract it directly from the base model, since it already contains lots of latent classification capabilities.
The central ingredient to the algorithm is Mutual Predictability Scoring (MPS): you try to find a set of labels (for your classification task datapoints) such that each label seems very likely (to the base model) given all other ones. A side ingredient are some cheap logical consistency checks that prevent some degenerate solutions (like all labels being the same one). It’s super expensive to find the optimum, but it turns out you can approximate it well with simulated annealing.
Learning the prior (LTP) is a scheme proposed by Christiano to get a classification model to generalize “closer to how a human would”, by training in a way that sticks closer to the human prior. This is better for safety, especially when considering generalization to superhuman tasks (which is also a motivation behind Unsupervised elicitation).
Here’s how it works: instead of simply training a model on a supervised set (x, y), we optimize an object z (say, a long list of claims) to satisfy two constraints:
z is likely according to the human prior. That is, the claims look something like “huskies are big dogs and tomatoes are red”, rather than something like “tomatoes are red and tomatoes are not red and the moon is made of cheese”
A human using z as a guide would correctly classify your (x,y) ground truth datapoints (for example, classify a red round vegetable as a tomato).
Of course, it’s too expensive to always use humans in both of these steps, so instead we should train some models Mprior(z) and M(y|x,z) to imitate humans.
In Unsupervised elicitation, you are basically finding this z, using a base model as a proxy for the human prior! In your case, z is “a long list of labelled classification tasks” (rather than more general kinds of claims talking about the classification task).
The other big difference is the existence of ground truth. Learning the prior does assume an existing (x,y) ground truth dataset (and thinks about generalization to other datasets), while you assume an empty dataset (and think about generalization to a non-empty dataset!). The reason you can do the latter is that the base model has enough latent capabilities for solving the (x,y) task, because it is a very salient task in human text (for example, assessing whether a piece of text was helpful). As a consequence, both MPS and the consistency checks are really being used to assess Mprior(z), and there is no check M(y|x,z) against any ground truth (x,y).
If instead we were dealing with a very non-salient task (very alien to humans), but still wanted the model to generalize OOD “as a human would”, then we would need to inject some supervised (x,y) examples. Then, you would actually want to use MPS to satisfy the constraint (2). One way to do this is to always include these (x,y) in your computation of MPS, even if they aren’t present in z.
Promising follow-ups
Injecting ground truth: As discussed in the previous paragraph. This one you already mention in your paper, but I think the LTP framing above helps clarify what we’re trying to do here! This would help us disentangle the base model’s existing knowledge about the salient task, and its bias generalizing from an alien task. And instead of everything being Mprior(z), we would also have M(y|x,z).
n-way assumptions: Geoffrey highlighted a limitation to LTP: the human (or Mprior) is supposed to assess the probability of z as a whole, which would be unrealistic when z gets very long. For example, if you give a human a list with a thousand claims and ask them to classify a datapoint, they might not be able to actually integrate all that information into their decision. The Anthropic paper faced an analogous constraint: the base model’s context length when doing MPS!
It would be nice if our subject matters satisfied an “n-way assumption”: regardless of the size of z, Mprior(z) can always be computed as a combination of the n-way interactions of its member claims Mprior(z1, …, zn). This way, the human wouldn’t need to ever go over context length n. This would be more likely if, for example, all contradictions were local enough: if z has a contradiction, then we can point it out by showing at most n of its claims.
In Anthropic’s paper, the Mprior(z) check includes the very weak logical check (just literal logical inconsistency), which is 2-way (you can always find A and notA, or A > B and A < B), and MPS, which might not even be n-way. Of course, it is actually a quantitative claim how strongly the n-way assumption holds, and it could be that the marginal benefit from larger interactions quickly plummets (even if not to 0).
It would be interesting to empirically test n-way assumptions! We could apply your algorithm with different n (not always maxing out the context, but sometimes also purposefully keeping it smaller), and compare performance.
This could be done in your existing settings, but more complex ones could also reveal more interesting structure. Take for example the task of saying, given a short text, which of a list of 20 vibes apply to it (like “joyful”, “written by an old person”, “formal”, etc.). This case can still be done without ground truth. But we can also encode the vibes’ names (so that they’re called “blorb”, “glop”, etc.), in which case the base model would require some ground truth (otherwise it doesn’t know what blorb is).
We could also develop elicitation algorithms that test n-way assumptions on the run, and change their hyperparameters accordingly. Especially since you might not always want to max out the context: sometimes a longer context will degrade model performance, or you can get away with shorter context which is faster.
Actually, base models don’t exactly do conditioning: In LTP, when computing M(y|x,z), we’d like the human to assume all of z as true. Even ignoring length problems, it might not be straightforward for a human to truly internalize and reason from arbitrary axioms. Similarly, it is not true that conditioning a base model on a long list of claims is exactly the same as “making the base model assume them as true”. Rather, it is closer to “making the base model assume that the author of this text has written them down”. Luckily, in many cases these two become the same, because the model learns to assume that the author is an intelligent truth-seeking human. But it would still be interesting to study cases where this doesn’t hold (and interferes with your algorithm), and consider other techniques for conditioning the base model, including whitebox methods.
More general z: As mentioned above, LTP was framed with z being any kind of claims about the classification task, while in your case it is always labelled classification tasks. It would be interesting to consider cases that could benefit from more general claims, and design algorithms that can find them! Of course, we won’t be able to just “optimize over token strings”, so we will need at least some kind of structure for our search.
Consider again the example above about vibes with encoded names. Here, having a long list of examples seems wasteful: it would instead be great if z could just be “blorb means happy, glop means formal, etc.”. But how can we find such meanings? We cannot just search over all adjectives or explanations, so we will need an algorithm that somehow queries the model for these explanations, and then checks how jointly consistent they are, and how well they do in predicting ground truth. In fact, there might exist a very general version of this algorithm, where the base model proposes arbitrary explanations that could help solve the problem (after seeing a bunch of solutions). My example is a bit unrealistic in that each encoded word corresponds to a very salient concept vibe that the model recognizes, but we could come up with more complicated settings.
It would be interesting to develop such an algorithm, and check whether the simple explanations it converges to tend to be the obvious ones to humans, or rather some inscrutable string uber-optimized for simplicity.
