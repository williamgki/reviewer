Large Language Models and the Critical Brain Hypothesis
David Demitri Africa, July 15, 2025
Summary:
I argue for my personal picture of developmental interpretability. I think the most useful way to study and control frontier-scale language models is to treat their training as a sequence of physical phase transitions. Singular learning theory conjectures that large networks move through critical periods in which their behaviour changes qualitatively. The Critical Brain Hypothesis describes a similar regime. Remarkably, transformers display parallel signatures: in-context learning appears abruptly once context length crosses a threshold, single fine-tune steps or jailbreak prompts can flip global behaviour, and “grokking” produces a critical slowing-down familiar from continuous phase transitions. This seems deeply connected to findings from CBH, and I discuss what this might imply.
Phase Transitions
Developmental interpretability is this really exciting research agenda that proposes we use singular learning theory (SLT) to study the development of language models in terms of phase transitions. If we want to study models much larger than the ones we have today, we should study the common phases they share in their development rather than their individual idiosyncrasies and quirks. Further, we should expect phase transitions to be easy to observe, even without using SLT tools; by definition, a phase transition should be obvious (and hopefully highly visible, when measuring certain indicators). This should make building a catalogue for engineering-style interventions somewhat easier. Finally, phase transitions seem especially relevant for control purposes. Within a phase, model behavior changes slowly and is easy to predict. In transition, models might exhibit unpredictable behavior which could be erratic or dangerous, and are generally more malleable which might make them susceptible to adversarial settings.
The Critical Brain Hypothesis (CBH) and Alignment
I think the Critical Brain Hypothesis is a useful and maybe even critical lens to understand developmental interpretability.

The CBH claims that biological brains spend most of their time in the narrow “edge-of-order-and-disorder” regime that statistical physicists would call a continuous phase transition. In that sliver of phase space, we notice a couple of interesting things. First, correlation length diverges, which means that local perturbations are less local, and propagate system-wide. Second, dynamic range is maximal, which is to say that the same substrate can represent both very small and very large signals. Finally, magnetic susceptibility peaks. The system is more sensitive to inputs and can switch global states with minimal energy.

This is uncannily similar to some observations we might make about LLMs. First, long-range credit assignment suddenly appears at the “emergent” context window length. Critical windows also show up during sampling, where narrow time intervals are heavily determinative of features that show up in the final output. Second, a single gradient update or prompt sometimes can flip the model from “helpful assistant” to jailbreak mode. Finally, grokking and related phenomena show loss curves with critical slow-downs and finite-size scaling, even after long stable training plateaus.

We might use the CBH as a principled umbrella under which emergence anecdotes might be re-organised: many of the qualitative jumps we see during scaling are actually the thermodynamic phase changes of a high-dimensional learning dynamical system. This might also provide a neat analogy to classical machine learning: data scientists used to carefully balance between the bias and variance terms, worried they might overparametrize their model. They had to pick the best model that wasn’t too complicated for the job. When neural networks scaled up by a lot, we found that it worked because once we had more parameters than datapoints, the space of functions we could interpolate through blew up and we could find really simple solutions after some period of double descent (where the loss spiked again, before converging to an even lower plateau). In this sense we might think of the period of interpolation through really overfitted solutions as the critical period, except it stays there all the time since the problem our model interpolates through is a constantly moving world which introduces constantly changing datapoints.

Once you admit the possibility that a transformer keeps skating along this surface, alignment becomes a matter of state-space control. We can reframe the training loop as a sequence of movements toward and away from the critical manifold. In sub-critical phases we harvest stability and low loss on easy data. In super-critical bursts we encourage exploration and task transfer. In the critical window we administer value-laden nudges to do alignment-related things. If this picture is correct, then safety interventions delivered elsewhere are less cost-effective and don’t make as much sense. I am not sure if it would make us more or less confident about monitoring CoT.

A natural objection is that seeming power-law behaviour in neural networks might be an artefact of finite size or unmodelled heterogeneity rather than evidence of criticality. That is a real possibility, and the history of criticality claims in biology is littered with premature conclusions. But for alignment purposes the burden of proof is asymmetrical. If even a subset of the observed “phase-transition” stories in large models turns out to be authentic, then ignoring them leaves safety gains on the table. Conversely, making training protocols conditional on criticality metrics is low-risk. If the metrics turn out to be red herrings, they will at worst add benign regularisation and interpretability checkpoints. 

You might also carry the objection that brains do a very different type of computation from large language models, and so we cannot analogize from one to another. I think that is reasonable, and it seems that brains are hardcoded in many different and idiosyncratic ways which neural networks don’t replicate. But my low confidence guess would be that self-organized criticality is a characteristic of generally intelligent systems, like how many companies run their supply chains with just-in-time inventory or how many financial markets hover near the edge of liquidity crises. 
Critical Daydreaming
Several people (Will, Geoffrey) have mentioned that this is related to LLM daydreaming, which I agree with. I imagine that the day-dreaming loop suggested here is kind of like criticality in slow-motion, where the loop asks the model to keep doing cheap background probes, and stores the result if it seems interesting. CBH would suggest that such nudges create useful global changes when the system is critical, and the system is incentivized to burn some extra compute to stay near this edge. If the CBH story is right, the day-dreaming loop is actually what any agent at criticality will end up doing to stay ready for its next phase change. Although, there is some difference in the story depicted, where the loop suggested moves through activation space since weights are frozen and critical surfing (below) would also move through weight space.

If you further take this perspective, then you might even think of this as a criticality tax. Extra compute is spent on noisy internal exploration because this keeps the representational landscape loose and re-writable, all so that it is easier to fuse distant ideas, invent new tools, or reinterpret your existing goals. This, I imagine, is why human insight feels stochastic at times. You think very hard on some research problem, and then go for a run or for lunch or for a climb (paying the tax). Then you come back after having the problem untangle in the back of your mind, and it strikes you like a bolt of lightning. I guess these are behavioral tricks so that you can hover near the phase boundary long enough for a useful avalanche to occur. 
Critical Surfing and some practical thoughts
Critical systems are simultaneously a boon and a bane: high susceptibility means small alignment gradients go a long way which makes learning easier, and also means mis-specifications and malicious inductive biases. So, this should imply some things about how we treat alignment.

What I think should follow is a control theory of “critical surfing”:
Detect when the model is entering a critical window (lambda spike, Hessian flattening, gradient non-Gaussianity).
Switch to an “alignment-first curriculum”. Here I think you could do some consistency training if it proves to be good for alignment, or some really high quality RLHF.
Exit criticality by regularisation once we satisfy certain safety constraints.

A few ways to verify this come to mind. We might check if the correlation length of representations scales with model depth. Second, we can look for “activation avalanches,” i.e. contiguous cascades of large activations across layers or timesteps; if the criticality picture is correct, their size distribution should approximate a power law with an exponent about 1.5 just before new capabilities emerge. Third, we can run matched fine-tuning experiments. I would guess that models that are nudged while still inside the critical window should need an order of magnitude (roughly ten- to one-hundred-fold) less RLHF reward signal to attain a given alignment metric, whereas the same intervention applied after convergence should leave them more brittle to off-distribution probes.

This leaves us with a couple of open questions still. Two that occur to me: First, can we deliberately widen or narrow the critical window? In pre-training, messing with the temperature scheduling and injecting noise sound like obvious ways to do this. I don’t have as good of a sense for how to do this in post-training, but maybe for inference we can maybe induce massive activations in the model or intervene in the sampling process. Second, is there some multiplexing of criticality going on? That is, are there nested phase transitions (syntax → semantics → meta-reasoning) that require different interventions at different epochs? And so on.

I think that capabilities arms races all but guarantee that future frontier models will be trained at scales where new critical windows appear (think: latent-planner, tool-use, multi-modal fusion). I think mastering the agentic loop is the next big one. Then maybe self-improvement, or online learning, or self-preservation, or a sense of self. It’s hard to tell. Regardless, if we think of models as being on the edge of criticality the entire time, then maybe the entire thing is a phase transition after a certain point. 
