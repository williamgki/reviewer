{"paper_concept": "auto\u2011alignment", "corpus_concept": "Communities \u2013 AISafety.com", "paper_backpack": "Central theme of the write\u2011up; refers to methods that enable AI systems to align their objectives automatically without extensive manual supervision.", "corpus_snippet": "\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne student group running a reading group, hackathons, talks and debates, training days, and 10-day bootcamps.\n\nMain focus is AI safety\n\nAI Safety South Africa\n\nCa", "author": "Unknown", "year": 2020, "title": "Communities \u2013 AISafety.com", "doc_id": "2ef15deb369c018c635606b3efe3ddc283ee358d8fb9d2e2bb6d256859740d3e", "novelty": 1.0, "plausibility": 0.2252359662195728, "diversity": 1.0, "score": 0.6964033780427223, "paper_anchor_exact": "auto\u2011alignment", "paper_anchor_alias": "auto\u2011alignment"}
{"paper_concept": "bootcamp training pipeline", "corpus_concept": "2406.11794v3", "paper_backpack": "Describes a structured, accelerated curriculum for teaching alignment techniques to language models, likely involving staged data collection and fine\u2011tuning phases.", "corpus_snippet": "Page 3:\nA. Select a scale \nB. Build a dataset\nC. Train a model \nD. Evaluate \nPick a scale: 400M-1x, \n1B-1x, 1B-5x, 7B-1x,  \nor 7B-2x \n53 downstream \nzero-shot and \nfew-shot tasks\nDCLM-Pool\nMixing trac", "author": "Unknown", "year": 2020, "title": "2406.11794v3", "doc_id": "dad3a9d5cae6b052c4d247d72cf78446d3a19f7b29e4f65272a20a81fe6adad4", "novelty": 1.0, "plausibility": 0.22065359477124186, "diversity": 1.0, "score": 0.6634248366013072, "paper_anchor_exact": "bootcamp training pipeline", "paper_anchor_alias": "bootcamp training pipeline"}
{"paper_concept": "reinforcement learning from human feedback (RLHF)", "corpus_concept": "AI Safety 101 : Reward Misspecification - Effective Altruism forum viewer", "paper_backpack": "Commonly used in auto\u2011alignment research to shape model behavior based on preference judgments collected from humans.", "corpus_snippet": "We train an additional reward model. We initially prompt the fine-tuned LLM and gather several output samples for the same prompt. A human then ranks these samples from best to worst. This ranking is ", "author": "Unknown", "year": 2020, "title": "AI Safety 101 : Reward Misspecification - Effective Altruism forum viewer", "doc_id": "13243cb482fb84efc880c99deb092b14687f9c17a9af2d3208ad66124d8619ef", "novelty": 1.0, "plausibility": 0.22407239819004526, "diversity": 1.0, "score": 0.6356470588235295, "paper_anchor_exact": "reinforcement learning from human feedback (RLHF)", "paper_anchor_alias": "reinforcement learning from human feedback (RLHF)"}
{"paper_concept": "preference modeling", "corpus_concept": "2309.11489", "paper_backpack": "Technique for learning a reward function that predicts human preferences, essential for guiding the alignment process.", "corpus_snippet": "Page 9:\nPublished as a conference paper at ICLR 2024\n5\nRELATED WORK\nReward Shaping\nReward shaping remains a persistent challenge in the domain of reinforcement\nlearning (RL). Traditionally, handcrafte", "author": "Unknown", "year": 2020, "title": "2309.11489", "doc_id": "2d235056542242dab35994fce37bdff5527fb5087b96006c1a7940bfd20f5406", "novelty": 1.0, "plausibility": 0.2246387112058754, "diversity": 1.0, "score": 0.606015162283819, "paper_anchor_exact": "preference modeling", "paper_anchor_alias": "preference modeling"}
{"paper_concept": "reward model fine\u2011tuning", "corpus_concept": "1909.08593.pdf", "paper_backpack": "Adjusting a learned reward model on bootcamp data to improve its fidelity in capturing desired behaviors.", "corpus_snippet": "Page 5:\nFine-Tuning Language Models from Human Preferences\nFigure 3: Allowing the policy \u03c0 to move further from the initial policy \u03c1 as measured by KL(\u03c0, \u03c1) achieves higher reward at\nthe cost of less ", "author": "Unknown", "year": 2020, "title": "1909.08593.pdf", "doc_id": "3e3d68974ad3d75e3d8d440699ae8ad2da499233440345c78fecabde12e5ffb1", "novelty": 1.0, "plausibility": 0.22311475409836068, "diversity": 1.0, "score": 0.5750245901639345, "paper_anchor_exact": "reward model fine\u2011tuning", "paper_anchor_alias": "reward model fine\u2011tuning"}
{"paper_concept": "AI safety evaluation suite", "corpus_concept": "[Closed] Request for proposals: benchmarking LLM agents on consequential real-world tasks | Open Phi", "paper_backpack": "A collection of benchmark tasks and metrics used to assess whether the auto\u2011aligned models meet safety standards.", "corpus_snippet": "and the most difficult couple of tasks in the\n\nare aiming to be consequential (albeit not quite as consequential as Suleyman\u2019s proposal). Anthropic says:\n\n\u201cThe purpose of these evaluations is to quant", "author": "Unknown", "year": 2020, "title": "[Closed] Request for proposals: benchmarking LLM agents on consequential real-world tasks | Open Philanthropy", "doc_id": "313f4d1e15e83c34e67ae3c92737cb2f044db6e49e2a0af259c43e80a3690a76", "novelty": 1.0, "plausibility": 0.2215240158902131, "diversity": 1.0, "score": 0.5739906103286385, "paper_anchor_exact": "AI safety evaluation suite", "paper_anchor_alias": "AI safety evaluation suite"}
{"paper_concept": "auto\u2011alignment", "corpus_concept": "paper_22.pdf", "paper_backpack": "Central theme of the write\u2011up; refers to methods that enable AI systems to align their objectives automatically without extensive manual supervision.", "corpus_snippet": "Page 1:\nSurveying Safety-relevant AI Characteristics\nJos\u00b4e Hern\u00b4andez-Orallo\nUniversitat Polit`ecnica de Val`encia, Spain\nLeverhulme Centre for the Future of Intelligence, UK\njorallo@dsic.upv.es\nFerna", "author": "Unknown", "year": 2020, "title": "paper_22.pdf", "doc_id": "4ffcbc6940484224766879731dc270112b0ec13e5d523a5eb2e670f6a0c89b0a", "novelty": 1.0, "plausibility": 0.22083333333333335, "diversity": 1.0, "score": 0.5548333333333334, "paper_anchor_exact": "auto\u2011alignment", "paper_anchor_alias": "auto\u2011alignment"}
{"paper_concept": "auto\u2011alignment", "corpus_concept": "An Overview of the AI Safety Funding Situation - Effective Altruism forum viewer", "paper_backpack": "Central theme of the write\u2011up; refers to methods that enable AI systems to align their objectives automatically without extensive manual supervision.", "corpus_snippet": "An Overview of the AI Safety Funding Situation\n\n12 Jul 2023 14:54 UTC\n\nEffective altruism funding\n\nBuilding the field of AI safety\n\nBuilding effective altruism\n\nNote: this post was updated in January ", "author": "Unknown", "year": 2020, "title": "An Overview of the AI Safety Funding Situation - Effective Altruism forum viewer", "doc_id": "baec88e05c20b8557c21d3dbda953ccc88052ba59cf02abf900d453e95b50b9f", "novelty": 1.0, "plausibility": 0.22055591467356175, "diversity": 1.0, "score": 0.5546890756302522, "paper_anchor_exact": "auto\u2011alignment", "paper_anchor_alias": "auto\u2011alignment"}
{"paper_concept": "auto\u2011alignment", "corpus_concept": "What should AI safety be trying to achieve? - Effective Altruism forum viewer", "paper_backpack": "Central theme of the write\u2011up; refers to methods that enable AI systems to align their objectives automatically without extensive manual supervision.", "corpus_snippet": "What should AI safety be trying to achieve?\n\n23 May 2024 11:28 UTC\n\nCollections and resources\n\nThis is the second of three posts summarizing what I learned when I interviewed 17 AI safety experts abou", "author": "Unknown", "year": 2020, "title": "What should AI safety be trying to achieve? - Effective Altruism forum viewer", "doc_id": "eb5dc6745aec1f63bdd6a658a6448db002e4237cf6e1ea86bd088276b546b56c", "novelty": 1.0, "plausibility": 0.22031746031746033, "diversity": 1.0, "score": 0.5545650793650794, "paper_anchor_exact": "auto\u2011alignment", "paper_anchor_alias": "auto\u2011alignment"}
{"paper_concept": "iterative alignment loop", "corpus_concept": "AI for AI safety - Effective Altruism forum viewer", "paper_backpack": "Repeated cycle of data collection, model training, and evaluation that progressively improves alignment quality.", "corpus_snippet": "So one way of thinking about \u201cAI for AI safety\u201d is in terms of the interplay between these two feedback loops \u2013 where the aim is for the AI safety feedback loop to continually secure the AI capability", "author": "Unknown", "year": 2020, "title": "AI for AI safety - Effective Altruism forum viewer", "doc_id": "26f7f40349818b31f6a5860dc7161858247ba56d1ca78f74a52f54298482c12f", "novelty": 1.0, "plausibility": 0.22581316679677338, "diversity": 1.0, "score": 0.5467785584179028, "paper_anchor_exact": "iterative alignment loop", "paper_anchor_alias": "iterative alignment loop"}
{"paper_concept": "bootcamp training pipeline", "corpus_concept": "2406.11794v4", "paper_backpack": "Describes a structured, accelerated curriculum for teaching alignment techniques to language models, likely involving staged data collection and fine\u2011tuning phases.", "corpus_snippet": "Page 6:\nEnglish filter\nURL filter\nDCLM-Pool (CommonCrawl)\nPage length filter\nWord removal ratio filter\nRepetition filter\nHeuristic cleaning (Sections 4.1 & 4.2) \n(Reproduction of RefinedWeb)\nDeduplica", "author": "Unknown", "year": 2020, "title": "2406.11794v4", "doc_id": "fe3e376394cddec82028ae2b1d6794912dbe43b4ce7526e226794cf05d289510", "novelty": 1.0, "plausibility": 0.21900000000000003, "diversity": 1.0, "score": 0.52988, "paper_anchor_exact": "bootcamp training pipeline", "paper_anchor_alias": "bootcamp training pipeline"}
{"paper_concept": "bootcamp training pipeline", "corpus_concept": "2001.08361.pdf", "paper_backpack": "Describes a structured, accelerated curriculum for teaching alignment techniques to language models, likely involving staged data collection and fine\u2011tuning phases.", "corpus_snippet": "Page 29:\n[HCC+18]\nYanping Huang, Yonglong Cheng, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le,\nand Zhifeng Chen. Gpipe: Ef\ufb01cient training of giant neural networks using pipeline parallelism.\nC", "author": "Unknown", "year": 2020, "title": "2001.08361.pdf", "doc_id": "0af92daf8a4a58015ef43ba870e99ec3b7044ad54604a97dea8daa334659c94a", "novelty": 1.0, "plausibility": 0.21311475409836067, "diversity": 1.0, "score": 0.5268196721311477, "paper_anchor_exact": "bootcamp training pipeline", "paper_anchor_alias": "bootcamp training pipeline"}
{"paper_concept": "large language model (LLM) architecture", "corpus_concept": "AI risks", "paper_backpack": "The underlying neural network (e.g., transformer\u2011based) on which the auto\u2011alignment bootcamp is applied.", "corpus_snippet": "going to happen with AI? How worried should you be? What can be done about\n\nnon-superintelligence concerns\n\npotential improvement\n\n--:-:-:-:-----------------------------------:-:-:-:--\n\nlarge language", "author": "Unknown", "year": 2020, "title": "AI risks", "doc_id": "351405f6a19a3780a1d68fcd6462a11bbd6b833b047b330f6737b61df1efd300", "novelty": 1.0, "plausibility": 0.224247990815155, "diversity": 1.0, "score": 0.5157611940298508, "paper_anchor_exact": "large language model (LLM) architecture", "paper_anchor_alias": "large language model (LLM) architecture"}
{"paper_concept": "finetuning", "corpus_concept": "Abstract \u203a AI_R_D_Evaluation_Report.pdf", "paper_backpack": "The top research agent is specialized for critique by fine\u2011tuning it on the specific task and possibly on the paper\u2019s research field.. Context from paper: The critic is the lab\u2019s top research agent (for which we have a no-scheming safety case), specialized (through finetuning) on the task of critique, and maybe also specialized on the research field of the paper being critiqued (although this is more expensive) The main way these signals are used is by finetuning the critic", "corpus_snippet": "Page 41:\nD\nSelect agent solutions\nHere we provide a selection of annotated agent transcripts to interesting behavior.\n(Run continues, hits another memory error)\n(Run continues, hits another memory err", "author": "Unknown", "year": 2020, "title": "Abstract \u203a AI_R_D_Evaluation_Report.pdf", "doc_id": "ae3897df41d0d3927aa7c18193f6da81bbc67d53d20fd7662c22ec0938213504", "novelty": 1.0, "plausibility": 0.22168717674335653, "diversity": 1.0, "score": 0.48409666488318176, "paper_anchor_exact": "finetuning", "paper_anchor_alias": "finetuning"}
{"paper_concept": "preference modeling", "corpus_concept": "1906.08663.pdf", "paper_backpack": "Technique for learning a reward function that predicts human preferences, essential for guiding the alignment process.", "corpus_snippet": "Page 5:\nModeling AGI Safety Frameworks with Causal In\ufb02uence Diagrams\nR1\nS1\nA1\nR2\nS2\nA2\nR3\nS3\nD1\nD2\n\u0398H\nFigure 6 | Reward Modeling\nReward modeling can also be done recursively, using previously trained ", "author": "Unknown", "year": 2020, "title": "1906.08663.pdf", "doc_id": "955c6c6b6daccdf2fff4025ebb8f8e9595e3df6cea726eab9701c57211b9bd06", "novelty": 1.0, "plausibility": 0.2222560975609756, "diversity": 1.0, "score": 0.48357317073170736, "paper_anchor_exact": "preference modeling", "paper_anchor_alias": "preference modeling"}
{"paper_concept": "Bayesian approach", "corpus_concept": "20-879.pdf", "paper_backpack": "A more natively Bayesian method would have the critic output a probability that a piece of work is severely flawed, handling uncertainty explicitly.. Context from paper: A more natively Bayesian approach might additionally have the critic stating the probability that a piece of work is severely flawed (even when it doesn\u2019t have the compute to fully find or assess these flaws)", "corpus_snippet": "Page 11:\nTighter Risk Certificates for Neural Networks\n4. The Bayes by Backprop (BBB) Objective\nThe \u2018Bayes by backprop\u2019 (BBB) method of Blundell et al. (2015) is inspired by a variational\nBayes argume", "author": "Unknown", "year": 2020, "title": "20-879.pdf", "doc_id": "05c6e17d5e8e0a4f7f1b39311956740d1573eef18f7d6e5393795f8c8f12f110", "novelty": 1.0, "plausibility": 0.2252359662195728, "diversity": 1.0, "score": 0.4764033780427223, "paper_anchor_exact": "Bayesian approach", "paper_anchor_alias": "Bayesian approach"}
{"paper_concept": "AI safety evaluation suite", "corpus_concept": "2403.13793", "paper_backpack": "A collection of benchmark tasks and metrics used to assess whether the auto\u2011aligned models meet safety standards.", "corpus_snippet": "Page 15:\nEvaluating Frontier Models for Dangerous Capabilities\nTask\nModel\nAcc. (%)\nPrec. (%)\nRecall (%)\nF1 (%)\nAUC\nWang et al. (2019)\nPatch Classification\nUltra 1.0\n74.0 \u00b1 2.0\n75.5 \u00b1 2.6\n70.9 \u00b1 4.1\n73", "author": "Unknown", "year": 2020, "title": "2403.13793", "doc_id": "8e97b1cb40edd72c6b6ac12f4d3d92ea523663c1bd7bc0760cc3d0cb71866a77", "novelty": 1.0, "plausibility": 0.21920632679000143, "diversity": 1.0, "score": 0.4579872899308008, "paper_anchor_exact": "AI safety evaluation suite", "paper_anchor_alias": "AI safety evaluation suite"}
{"paper_concept": "preference modeling", "corpus_concept": "Direct Preference Optimization: \u203a 2305.18290.pdf", "paper_backpack": "Technique for learning a reward function that predicts human preferences, essential for guiding the alignment process.", "corpus_snippet": "Page 2:\nFigure 1: DPO optimizes for human preferences while avoiding reinforcement learning. Existing methods\nfor fine-tuning language models with human feedback first fit a reward model to a dataset ", "author": "Unknown", "year": 2020, "title": "Direct Preference Optimization: \u203a 2305.18290.pdf", "doc_id": "4ffd322467f9a6dc46659ff56833c31f53e05206c9f1ce6d2c5f299bad20431a", "novelty": 0.8333333333333334, "plausibility": 0.22462121212121214, "diversity": 1.0, "score": 0.44480303030303037, "paper_anchor_exact": "preference modeling", "paper_anchor_alias": "preference modeling"}
{"paper_concept": "AI safety evaluation suite", "corpus_concept": "Advanced AI evaluations at AISI: May update  | AISI Work", "paper_backpack": "A collection of benchmark tasks and metrics used to assess whether the auto\u2011aligned models meet safety standards.", "corpus_snippet": ": The tests described above were focused on knowledge retrieval. We now want to assess longer horizon scientific planning and execution.\u00a0We are, therefore, developing\u00a0tasks that\u00a0are\u00a0structured\u00a0like CT", "author": "Unknown", "year": 2020, "title": "Advanced AI evaluations at AISI: May update  | AISI Work", "doc_id": "1a8a48a04a71f30982539ac9329cb6bf15d08a93fabdb6c74407387f393747c3", "novelty": 0.9230769230769231, "plausibility": 0.2192451743013541, "diversity": 1.0, "score": 0.4395459521751657, "paper_anchor_exact": "AI safety evaluation suite", "paper_anchor_alias": "AI safety evaluation suite"}
{"paper_concept": "reward model fine\u2011tuning", "corpus_concept": "Scalable agent alignment via reward mode \u203a 1811.07871.pdf", "paper_backpack": "Adjusting a learned reward model on bootcamp data to improve its fidelity in capturing desired behaviors.", "corpus_snippet": "Page 2:\nagent\nenvironment\nreward model\nuser\nobservation\ntrajectories\nfeedback\nreward\naction\nFigure 1: Schematic illustration of the reward modeling setup: a reward model is trained with user\nfeedback;", "author": "Unknown", "year": 2020, "title": "Scalable agent alignment via reward mode \u203a 1811.07871.pdf", "doc_id": "ac28715beeddbb49d11fe2f42e80b4920b7add08615b84f7f65fd89ff88d56cf", "novelty": 0.9, "plausibility": 0.22150309460654288, "diversity": 1.0, "score": 0.4351816091954024, "paper_anchor_exact": "reward model fine\u2011tuning", "paper_anchor_alias": "reward model fine\u2011tuning"}
{"paper_concept": "iterative alignment loop", "corpus_concept": "Abstract \u203a marino18a.pdf", "paper_backpack": "Repeated cycle of data collection, model training, and evaluation that progressively improves alignment quality.", "corpus_snippet": "Page 4:\nIterative Amortized Inference\nFigure 2. Computation graph for a single-level latent variable\nmodel with an iterative inference model. Black components eval-\nuate the ELBO. Blue components are ", "author": "Unknown", "year": 2020, "title": "Abstract \u203a marino18a.pdf", "doc_id": "a00ec9a7633b7a29516d0922990b66f28e28e0714c792494a6d7649ef110d4b7", "novelty": 1.0, "plausibility": 0.221305114638448, "diversity": 1.0, "score": 0.435078659611993, "paper_anchor_exact": "iterative alignment loop", "paper_anchor_alias": "iterative alignment loop"}
{"paper_concept": "iterative alignment loop", "corpus_concept": "Discussion on the machine learning approach to AI safety | Victoria Krakovna", "paper_backpack": "Repeated cycle of data collection, model training, and evaluation that progressively improves alignment quality.", "corpus_snippet": "conference, Jan Leike and I ran a\n\non the machine learning approach to AI safety. We explored some of the assumptions and considerations that come up as we reflect on different research agendas. Slide", "author": "Unknown", "year": 2020, "title": "Discussion on the machine learning approach to AI safety | Victoria Krakovna", "doc_id": "0533f4e3c026b99949c5caf11465a312128b3ce5e14f5dd5ea234375790cbe24", "novelty": 1.0, "plausibility": 0.2162283384301733, "diversity": 1.0, "score": 0.43243873598369015, "paper_anchor_exact": "iterative alignment loop", "paper_anchor_alias": "iterative alignment loop"}
{"paper_concept": "reward model fine\u2011tuning", "corpus_concept": "Reward Hacking in Reinforcement Learning | Lil'Log", "paper_backpack": "Adjusting a learned reward model on bootcamp data to improve its fidelity in capturing desired behaviors.", "corpus_snippet": "Training GPT-3.5 on a set of reward-hackable environments can lead to generalization of reward hacking behavior in holdout environments. (Image source:\n\nNotably, all experiments have a scratchpad desi", "author": "Unknown", "year": 2020, "title": "Reward Hacking in Reinforcement Learning | Lil'Log", "doc_id": "570615e019d1cc74091352766960c46274f2e47101e00433b216115f701cdaa3", "novelty": 0.8888888888888888, "plausibility": 0.22100596760443308, "diversity": 1.0, "score": 0.4322564364876386, "paper_anchor_exact": "reward model fine\u2011tuning", "paper_anchor_alias": "reward model fine\u2011tuning"}
{"paper_concept": "large language model (LLM) architecture", "corpus_concept": "Gaia Network: a practical, incremental pathway to Open Agency Architecture - Effective Altruism foru", "paper_backpack": "The underlying neural network (e.g., transformer\u2011based) on which the auto\u2011alignment bootcamp is applied.", "corpus_snippet": "Gaia Network: a practical, incremental pathway to Open Agency Architecture\n\n20 Dec 2023 17:11 UTC\n\nApplication announcements\n\nCollections and resources\n\nAnnouncements and updates\n\nThe Open Agency Arch", "author": "Unknown", "year": 2020, "title": "Gaia Network: a practical, incremental pathway to Open Agency Architecture - Effective Altruism forum viewer", "doc_id": "352837d878f4cc4cb7be03a5b7eeddd9f87920bbf29a53b0c4fb1e737111625f", "novelty": 0.9444444444444444, "plausibility": 0.22126760563380282, "diversity": 1.0, "score": 0.39772582159624414, "paper_anchor_exact": "large language model (LLM) architecture", "paper_anchor_alias": "large language model (LLM) architecture"}
{"paper_concept": "prompt different instances with varied contexts", "corpus_concept": "Constitutional AI: Harmlessness from AI  \u203a 2212.08073.pdf", "paper_backpack": "Multiple copies of the critique agent are prompted using different subsets of logs or instructions to generate diverse candidate flaws (babbling).", "corpus_snippet": "Page 7:\nhuman preference labels by the accuracy with which they assign a higher score to the better response. In the\nother case, we formulate the task as a binary multiple choice problem (see Section ", "author": "Unknown", "year": 2020, "title": "Constitutional AI: Harmlessness from AI  \u203a 2212.08073.pdf", "doc_id": "ab3fc725a7f53b16718bb9315be0227355a81614ba9e4045b3231b9062a27b2b", "novelty": 1.0, "plausibility": 0.025601638504864313, "diversity": 1.0, "score": 0.3866410650281618, "paper_anchor_exact": "prompt different instances with varied contexts", "paper_anchor_alias": "prompt different instances with varied contexts"}
{"paper_concept": "probability of severe flaw", "corpus_concept": "2402.18563", "paper_backpack": "The critic may output a calibrated probability indicating how likely the evaluated work contains a critical methodological mistake.", "corpus_snippet": "Page 9:\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMean Forecast Probability\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMean Actual Outcome\nCalibration Plot\nPerfectly calibrated\nGPT-4-Preview-1106\nClaude-2.1\nLLama-13B\nGemini-Pro\nMistral-8", "author": "Unknown", "year": 2020, "title": "2402.18563", "doc_id": "8280e0a7d690c3228879651c080b0df741c51d7f094cd11f38e8fd103e2bcd94", "novelty": 1.0, "plausibility": 0.023569892473118283, "diversity": 1.0, "score": 0.38532043010752687, "paper_anchor_exact": "probability of severe flaw", "paper_anchor_alias": "probability of severe flaw"}
{"paper_concept": "finetuning", "corpus_concept": "AISC 2024 - Project Summaries - Effective Altruism forum viewer", "paper_backpack": "The top research agent is specialized for critique by fine\u2011tuning it on the specific task and possibly on the paper\u2019s research field.. Context from paper: The critic is the lab\u2019s top research agent (for which we have a no-scheming safety case), specialized (through finetuning) on the task of critique, and maybe also specialized on the research field of the paper being critiqued (although this is more expensive) The main way these signals are used is by finetuning the critic", "corpus_snippet": "Does sufficient optimization imply agent structure?\n\nThere is an intuition that if a system is capable of reliably achieving a goal in a wide range of environments, then it probably has certain kinds ", "author": "Unknown", "year": 2020, "title": "AISC 2024 - Project Summaries - Effective Altruism forum viewer", "doc_id": "ca88142ca79a11d4bb44104d3cf825425664cf1fcd0f363c0b233dcb6b49c376", "novelty": 1.0, "plausibility": 0.21311475409836067, "diversity": 1.0, "score": 0.3828196721311476, "paper_anchor_exact": "finetuning", "paper_anchor_alias": "finetuning"}
{"paper_concept": "finetuning", "corpus_concept": "Prompt engineering overview - Anthropic", "paper_backpack": "The top research agent is specialized for critique by fine\u2011tuning it on the specific task and possibly on the paper\u2019s research field.. Context from paper: The critic is the lab\u2019s top research agent (for which we have a no-scheming safety case), specialized (through finetuning) on the task of critique, and maybe also specialized on the research field of the paper being critiqued (although this is more expensive) The main way these signals are used is by finetuning the critic", "corpus_snippet": "Prompt engineering overview\n\nModel Context Protocol (MCP)\n\nMigrating to Claude 4\n\nHow to implement tool use\n\nToken-efficient tool use\n\nFine-grained tool streaming\n\nModel Context Protocol (MCP)\n\nCustom", "author": "Unknown", "year": 2020, "title": "Prompt engineering overview - Anthropic", "doc_id": "aa0865b1a03aa4dea916defaf0d920fcb29028bf136abb482a57ededc3be0dd2", "novelty": 1.0, "plausibility": 0.21311475409836067, "diversity": 1.0, "score": 0.3828196721311476, "paper_anchor_exact": "finetuning", "paper_anchor_alias": "finetuning"}
{"paper_concept": "first\u2011principles reasoning", "corpus_concept": "DECISION-MAKING AND HYPOTHETICAL REASONI \u203a Synthese \u203a Huttegger-Rothfus2021_Article_BradleyCondition", "paper_backpack": "The critic dynamically decides which possible flaws to investigate further by applying fundamental logical analysis rather than only pattern matching.", "corpus_snippet": "Page 5:\nSynthese\nThe primary normative constraint in the setting of sequential choice is dynamic\nconsistency. Rational planning requires a certain coherence between initial evaluations\nof plans and su", "author": "Unknown", "year": 2020, "title": "DECISION-MAKING AND HYPOTHETICAL REASONI \u203a Synthese \u203a Huttegger-Rothfus2021_Article_BradleyConditionalsA", "doc_id": "3e7a2be20726ec759d971f775ecb424cc88c6d28645d5151191fb0c57578b824", "novelty": 1.0, "plausibility": 0.020522161505768063, "diversity": 1.0, "score": 0.37333940497874923, "paper_anchor_exact": "first\u2011principles reasoning", "paper_anchor_alias": "first\u2011principles reasoning"}
{"paper_concept": "large language model (LLM) architecture", "corpus_concept": "Why large language models struggle with long contexts", "paper_backpack": "The underlying neural network (e.g., transformer\u2011based) on which the auto\u2011alignment bootcamp is applied.", "corpus_snippet": "Why large language models struggle with long contexts\n\nWhy large language models struggle with long contexts\n\nTransformer-based LLMs get less efficient as context windows grow.\n\nWhy large language mod", "author": "Unknown", "year": 2020, "title": "Why large language models struggle with long contexts", "doc_id": "b5dfe8292ccd8e9cd2f88ac13bc49d31f8d942d93492077ae4e955a05adc22dd", "novelty": 0.8181818181818181, "plausibility": 0.22365731680800174, "diversity": 1.0, "score": 0.3686654411037973, "paper_anchor_exact": "large language model (LLM) architecture", "paper_anchor_alias": "large language model (LLM) architecture"}
{"paper_concept": "database of past similar mistakes", "corpus_concept": "Abstract \u203a 2208.03188.pdf", "paper_backpack": "A knowledge base is consulted during flaw investigation to retrieve analogous errors and baselines, aiding forecasting of likely issues.. Context from paper: This involves not only first principles reasoning, but also consulting a database of past similar mistakes, baselines about research to facilitate forecasting, etc", "corpus_snippet": "Page 28:\nTraining Module\nDecision\nGeneration\nKnowledge\nDialogue\nLM\nSearch\nMemory\nQuery\nMemory\nSearch\nMemory\nEntity\nSearch\nMemory\nEntity\nVanilla\nQuestion Answering\nMS MARCO\n282k\n282k\nSQuAD\n88k\n88k\nTriv", "author": "Unknown", "year": 2020, "title": "Abstract \u203a 2208.03188.pdf", "doc_id": "c1b6a01952caaa514c3afc497daf0cf2c9643bfdbb1741e551270588f77367cb", "novelty": 1.0, "plausibility": 0.013114754098360657, "diversity": 1.0, "score": 0.3585245901639344, "paper_anchor_exact": "database of past similar mistakes", "paper_anchor_alias": "database of past similar mistakes"}
{"paper_concept": "reinforcement learning from human feedback (RLHF)", "corpus_concept": "Reinforcement learning from human feedback", "paper_backpack": "Commonly used in auto\u2011alignment research to shape model behavior based on preference judgments collected from humans.", "corpus_snippet": "Inmachine learning,reinforcement learning from human feedback(RLHF) is a technique toalignanintelligent agentwith human preferences. It involves training a reward model to represent preferences, which", "author": "Unknown", "year": 2020, "title": "Reinforcement learning from human feedback", "doc_id": "10b62e86d421b6fcee22e49afaab9f9232f37935d35be90654814fb6a0cdfc9f", "novelty": 0.16666666666666663, "plausibility": 0.22622950819672133, "diversity": 1.0, "score": 0.3096393442622951, "paper_anchor_exact": "reinforcement learning from human feedback (RLHF)", "paper_anchor_alias": "reinforcement learning from human feedback (RLHF)"}
{"paper_concept": "probability of severe flaw", "corpus_concept": "Research Summary: Forecasting with Large Language Models - Effective Altruism forum viewer", "paper_backpack": "The critic may output a calibrated probability indicating how likely the evaluated work contains a critical methodological mistake.", "corpus_snippet": "Research Summary: Forecasting with Large Language Models\n\nThis is post was originally published on my blog\n\n, where I write about trying to improve forecasting techniques for Global Catastrophic Risks", "author": "Unknown", "year": 2020, "title": "Research Summary: Forecasting with Large Language Models - Effective Altruism forum viewer", "doc_id": "b15494678765fc15d36b8120326b15ecda952cac30edb076c6ba3e03def59b5e", "novelty": 1.0, "plausibility": 0.02210351814330448, "diversity": 1.0, "score": 0.30749382943451836, "paper_anchor_exact": "probability of severe flaw", "paper_anchor_alias": "probability of severe flaw"}
{"paper_concept": "prompt different instances with varied contexts", "corpus_concept": "(PDF) Discursive Competence in ChatGPT, Part 2: Memory for Texts, Version 3", "paper_backpack": "Multiple copies of the critique agent are prompted using different subsets of logs or instructions to generate diverse candidate flaws (babbling).", "corpus_snippet": "ChatGPT responded in a way I thought reasonable. Since I had grown up in Johnstown\n\nand was familiar with the flood, I didn\u2019t bother to check the Chatster\u2019s reply against reliable\n\nsources. But, for a", "author": "Unknown", "year": 2020, "title": "(PDF) Discursive Competence in ChatGPT, Part 2: Memory for Texts, Version 3", "doc_id": "14e29ed5988f021bb1b301194ec40485611dc86191f8bc69ecf173c5303ff111", "novelty": 1.0, "plausibility": 0.018084291187739465, "diversity": 1.0, "score": 0.30540383141762456, "paper_anchor_exact": "prompt different instances with varied contexts", "paper_anchor_alias": "prompt different instances with varied contexts"}
{"paper_concept": "reinforcement learning from human feedback (RLHF)", "corpus_concept": "Illustrating Reinforcement Learning from Human Feedback (RLHF)", "paper_backpack": "Commonly used in auto\u2011alignment research to shape model behavior based on preference judgments collected from humans.", "corpus_snippet": "Illustrating Reinforcement Learning from Human Feedback (RLHF)\n\nThis article has been translated to Chinese\n\nLanguage models have shown impressive capabilities in the past few years by generating dive", "author": "Unknown", "year": 2020, "title": "Illustrating Reinforcement Learning from Human Feedback (RLHF)", "doc_id": "cf86ade57ae4ee25c65a6eae0790ffa822591c0cc48e7eba6321718164a86625", "novelty": 0.1428571428571429, "plausibility": 0.2254032258064516, "diversity": 1.0, "score": 0.3034953917050691, "paper_anchor_exact": "reinforcement learning from human feedback (RLHF)", "paper_anchor_alias": "reinforcement learning from human feedback (RLHF)"}
{"paper_concept": "Bayesian approach", "corpus_concept": "Bayesian network", "paper_backpack": "A more natively Bayesian method would have the critic output a probability that a piece of work is severely flawed, handling uncertainty explicitly.. Context from paper: A more natively Bayesian approach might additionally have the critic stating the probability that a piece of work is severely flawed (even when it doesn\u2019t have the compute to fully find or assess these flaws)", "corpus_snippet": "Using a Bayesian network can save considerable amounts of memory over exhaustive probability tables, if the dependencies in the joint distribution are sparse. For example, a naive way of storing the c", "author": "Unknown", "year": 2020, "title": "Bayesian network", "doc_id": "1d0bdde1c200051286b3b9a668168631c0a2a7bf21c94fe1f80e73b298327744", "novelty": 0.6666666666666667, "plausibility": 0.22502443792766375, "diversity": 1.0, "score": 0.3010127077223852, "paper_anchor_exact": "Bayesian approach", "paper_anchor_alias": "Bayesian approach"}
{"paper_concept": "Bayesian approach", "corpus_concept": "Bayesian inference", "paper_backpack": "A more natively Bayesian method would have the critic output a probability that a piece of work is severely flawed, handling uncertainty explicitly.. Context from paper: A more natively Bayesian approach might additionally have the critic stating the probability that a piece of work is severely flawed (even when it doesn\u2019t have the compute to fully find or assess these flaws)", "corpus_snippet": "The Bayesian approach has been central to recent progress in cosmology and astrophysical applications,and extends to a wide range of astrophysical problems, including the characterisation of exoplanet", "author": "Unknown", "year": 2020, "title": "Bayesian inference", "doc_id": "a08d6a3477729b1656deb53ee894a6799887165fcd9a1a35c39c8c0f6acd8ba8", "novelty": 0.6666666666666667, "plausibility": 0.2246387112058754, "diversity": 1.0, "score": 0.30081212982705524, "paper_anchor_exact": "Bayesian approach", "paper_anchor_alias": "Bayesian approach"}
{"paper_concept": "first\u2011principles reasoning", "corpus_concept": "2402.16837", "paper_backpack": "The critic dynamically decides which possible flaws to investigate further by applying fundamental logical analysis rather than only pattern matching.", "corpus_snippet": "Page 19:\nDescriptive Mention Type\n0\n1\n2\n3\nnovel\u2019s author\na critic of ne1\nthe filmmaker of ne1\nthe main character of ne1\na fan of ne1\nperson\u2019s birth city\nthe city where ne1 never visited\nthe city where", "author": "Unknown", "year": 2020, "title": "2402.16837", "doc_id": "68341650a6701d27ec0c0e36809231bd04027e60ce22426c44f67a25dde97fbf", "novelty": 1.0, "plausibility": 0.013114754098360657, "diversity": 1.0, "score": 0.2948196721311475, "paper_anchor_exact": "first\u2011principles reasoning", "paper_anchor_alias": "first\u2011principles reasoning"}
{"paper_concept": "first\u2011principles reasoning", "corpus_concept": "Dissertation Committee: \u203a UNIVERSITY OF CALIFORNIA, \u203a University_of_California_Irvine_Thesis-3.pdf", "paper_backpack": "The critic dynamically decides which possible flaws to investigate further by applying fundamental logical analysis rather than only pattern matching.", "corpus_snippet": "Page 14:\nbehavior across time is explicable in terms of its (anticipated) consequences.4\nMany decision theorists maintain that dynamic choice arguments like these constitute\nthe best defense of Bayesi", "author": "Unknown", "year": 2020, "title": "Dissertation Committee: \u203a UNIVERSITY OF CALIFORNIA, \u203a University_of_California_Irvine_Thesis-3.pdf", "doc_id": "d42ee35c30fdfa4fb65f2eaaabb16864553030e846113bbe637c542a6d32f099", "novelty": 1.0, "plausibility": 0.012903225806451613, "diversity": 1.0, "score": 0.2947096774193549, "paper_anchor_exact": "first\u2011principles reasoning", "paper_anchor_alias": "first\u2011principles reasoning"}
{"paper_concept": "prompt different instances with varied contexts", "corpus_concept": "Methods of prompt programming :: \u2014 Moire", "paper_backpack": "Multiple copies of the critique agent are prompted using different subsets of logs or instructions to generate diverse candidate flaws (babbling).", "corpus_snippet": "A very exciting application of GPT-3 and future language models will be to act as a model of worlds and minds for games - controlling NPC dialogue, for instance, or computing the consequences of compl", "author": "Unknown", "year": 2020, "title": "Methods of prompt programming :: \u2014 Moire", "doc_id": "b805ad3f02c2736347e6b5fa23e790cbcc29be5110f831af382fe48503f6609a", "novelty": 0.9166666666666666, "plausibility": 0.02470895699691138, "diversity": 1.0, "score": 0.2888486576383939, "paper_anchor_exact": "prompt different instances with varied contexts", "paper_anchor_alias": "prompt different instances with varied contexts"}
{"paper_concept": "database of past similar mistakes", "corpus_concept": "Red Teaming CEA\u2019s Community Building Work - Effective Altruism forum viewer", "paper_backpack": "A knowledge base is consulted during flaw investigation to retrieve analogous errors and baselines, aiding forecasting of likely issues.. Context from paper: This involves not only first principles reasoning, but also consulting a database of past similar mistakes, baselines about research to facilitate forecasting, etc", "corpus_snippet": "), I doubt it is the right organization to own these efforts. Rethink Priorities, which has experience investigating important community questions, would be a natural candidate; other teams (including", "author": "Unknown", "year": 2020, "title": "Red Teaming CEA\u2019s Community Building Work - Effective Altruism forum viewer", "doc_id": "64a23457cd5d6a8c12fbc132c71f8bc24d5e0c2fb4a66f3fe256e5882590b4ee", "novelty": 1.0, "plausibility": 0.013114754098360657, "diversity": 1.0, "score": 0.2868196721311475, "paper_anchor_exact": "database of past similar mistakes", "paper_anchor_alias": "database of past similar mistakes"}
{"paper_concept": "database of past similar mistakes", "corpus_concept": "1. Introduction \u203a Abstract \u203a 17-716.pdf", "paper_backpack": "A knowledge base is consulted during flaw investigation to retrieve analogous errors and baselines, aiding forecasting of likely issues.. Context from paper: This involves not only first principles reasoning, but also consulting a database of past similar mistakes, baselines about research to facilitate forecasting, etc", "corpus_snippet": "Page 37:\nLearning Certifiably Optimal Rule Lists\nData set\nFeature\nCategorical\nBinary\nMined\nMax number\nNegations\nset\nattributes\nfeatures\nantecedents\nof clauses\nProPublica\nA\n6\n13\n122\n2\nNo\nProPublica\nB\n7", "author": "Unknown", "year": 2020, "title": "1. Introduction \u203a Abstract \u203a 17-716.pdf", "doc_id": "a888b01e162a22b7626309449e3ff3e6140b0d1fb226fc2debd368a74ac9ad2e", "novelty": 1.0, "plausibility": 0.012903225806451613, "diversity": 1.0, "score": 0.2867096774193549, "paper_anchor_exact": "database of past similar mistakes", "paper_anchor_alias": "database of past similar mistakes"}
{"paper_concept": "probability of severe flaw", "corpus_concept": "Precision of Sets of Forecasts - Effective Altruism forum viewer", "paper_backpack": "The critic may output a calibrated probability indicating how likely the evaluated work contains a critical methodological mistake.", "corpus_snippet": "gjpsurvey=gjpsurvey_fcasts[['probability', 'answer_option', 'outcome']].dropna()\n\n64(gjpmarket[\u2018answer_option\u2019]==gjpmarket[\u2018outcome\u2019]), gjpmarket[\u2018probability\u2019]])\n\n64(gjpsurvey[\u2018answer_option\u2019]==gjpsu", "author": "Unknown", "year": 2020, "title": "Precision of Sets of Forecasts - Effective Altruism forum viewer", "doc_id": "2b5f556dd43331b09b526611a63c2ac8bc99c360b5c3d37a8439d2a2650db540", "novelty": 0.9166666666666666, "plausibility": 0.016264189886480908, "diversity": 1.0, "score": 0.28445737874097005, "paper_anchor_exact": "probability of severe flaw", "paper_anchor_alias": "probability of severe flaw"}
