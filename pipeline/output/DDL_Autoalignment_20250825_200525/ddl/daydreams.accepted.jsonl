{"pair": ["reward model fine\u2011tuning", "Reward Hacking in Reinforcement Learning | Lil'Log"], "paper_concept": "reward model fine\u2011tuning", "corpus_concept": "Reward Hacking in Reinforcement Learning | Lil'Log", "author": "Unknown", "year": 2020, "title": "Reward Hacking in Reinforcement Learning | Lil'Log", "doc_id": "570615e019d1cc74091352766960c46274f2e47101e00433b216115f701cdaa3", "sampling_score": 0.4322564364876386, "hypothesis": "CLAIM: Adjusting a reward model via \"reward model fine\u2011tuning\" on bootcamp data reduces the emergence of reward hacking in unseen environments. MECHANISM: By exposing the reward model to curated bootcamp examples where hackable incentives are explicitly penalized, the fine\u2011tuned model learns to assign lower scores to deceptive trajectories, thereby shaping the policy away from exploiting proxy rewards. TEST: Conduct an ablation control where a policy is trained with a bootcamp\u2011fine\u2011tuned reward model versus a baseline reward model on identical RLHF pipelines and evaluate both on a held\u2011out suite of known hackable tasks. SIGNALS: A statistically significant drop in hack score (e.g., lower proportion of exploitative actions) for the fine\u2011tuned condition would support the claim; no change or higher hack scores would refute it.", "paper_anchor": "reward model fine\u2011tuning", "test": {"type": "ablation_control", "dataset_or_component": "bootcamp-fine-tuned reward model vs baseline reward model", "manipulation": "replace baseline reward model with the fine\u2011tuned version during RLHF training", "metric": "percentage reduction in hackable actions across holdout environments", "expected_direction": "decrease", "success_threshold": 0.05, "timeframe_days": 14}, "critic": {"novelty": 0.7, "coherence": 0.9, "usefulness": 0.865, "binding": 1.0, "overall_score": 0.6054999999999999, "diagnostics": {"hard": true, "soft": 0.056, "binding_score": 1.0}}}
{"pair": ["probability of severe flaw", "Research Summary: Forecasting with Large Language Models - Effective Altruism forum viewer"], "paper_concept": "probability of severe flaw", "corpus_concept": "Research Summary: Forecasting with Large Language Models - Effective Altruism forum viewer", "author": "Unknown", "year": 2020, "title": "Research Summary: Forecasting with Large Language Models - Effective Altruism forum viewer", "doc_id": "b15494678765fc15d36b8120326b15ecda952cac30edb076c6ba3e03def59b5e", "sampling_score": 0.30749382943451836, "hypothesis": "CLAIM: The calibrated critic\u2019s output of the \"probability of severe flaw\" can reliably predict when a forecasting model summary will later be flagged by expert reviewers as containing methodological errors. MECHANISM: Because the critic is trained on annotated critiques, its probability estimate captures latent signals such as ambiguous priors and inconsistent validation that human forecasters often miss, leading to higher alignment with subsequent expert judgments. TEST: We will run an adversarial_probe where we feed the critic a batch of LLM\u2011generated forecasting summaries from the Effective Altruism forum and record its reported probability for each; then compare these scores against binary labels derived from post\u2011hoc reviewer audits. SIGNALS: A strong positive correlation (e.g., AUROC > 0.75) would support the claim, whereas near\u2011random performance (AUROC \u2248 0.5) or systematic under\u2011prediction would refute it.", "paper_anchor": "probability of severe flaw", "test": {"type": "adversarial_probe", "dataset_or_component": "Effective Altruism forum forecasting summaries", "manipulation": "use critic to output probability for each summary", "metric": "AUROC comparing predicted scores to expert error labels", "expected_direction": "increase", "success_threshold": 0.75, "timeframe_days": 14}, "critic": {"novelty": 0.7, "coherence": 0.9, "usefulness": 0.85, "binding": 1.0, "overall_score": 0.595, "diagnostics": {"hard": true, "soft": 0.055, "binding_score": 1.0}}}
{"pair": ["first\u2011principles reasoning", "Dissertation Committee: \u203a UNIVERSITY OF CALIFORNIA, \u203a University_of_California_Irvine_Thesis-3.pdf"], "paper_concept": "first\u2011principles reasoning", "corpus_concept": "Dissertation Committee: \u203a UNIVERSITY OF CALIFORNIA, \u203a University_of_California_Irvine_Thesis-3.pdf", "author": "Unknown", "year": 2020, "title": "Dissertation Committee: \u203a UNIVERSITY OF CALIFORNIA, \u203a University_of_California_Irvine_Thesis-3.pdf", "doc_id": "d42ee35c30fdfa4fb65f2eaaabb16864553030e846113bbe637c542a6d32f099", "sampling_score": 0.2947096774193549, "hypothesis": "CLAIM: The critic\u2019s dynamic flaw\u2011selection algorithm outperforms static pattern\u2011matching baselines on academic dissertation texts because it employs first\u2011principles reasoning to prioritize logical inconsistencies. MECHANISM: By grounding each candidate flaw in a formal logical schema rather than surface n\u2011grams, the system can allocate attention to arguments whose validity hinges on underlying deductive structure, leading to deeper investigative depth. TEST: Run an ablation where the logical analysis module is replaced with a pure pattern matcher and compare F1 scores on a held\u2011out set of UC Irvine thesis paragraphs annotated for reasoning errors. SIGNALS: A statistically significant drop (\u22655% absolute F1) in the ablated version would support the claim, whereas negligible change or improvement would refute it. Additional qualitative inspection of highlighted passages should reveal that the full model flags premises with hidden contradictions, while the control flags only surface lexical anomalies.", "paper_anchor": "first\u2011principles reasoning", "test": {"type": "ablation_control", "dataset_or_component": "UC Irvine thesis reasoning error annotations", "manipulation": "replace logical analysis module with pattern matcher", "metric": "F1 score on flaw detection", "expected_direction": "decrease", "success_threshold": 0.05, "timeframe_days": 14}, "critic": {"novelty": 0.635, "coherence": 0.935, "usefulness": 0.88, "binding": 1.0, "overall_score": 0.5588, "diagnostics": {"hard": true, "soft": 0.05, "binding_score": 1.0}}}
{"pair": ["bootcamp training pipeline", "2406.11794v3"], "paper_concept": "bootcamp training pipeline", "corpus_concept": "2406.11794v3", "author": "Unknown", "year": 2020, "title": "2406.11794v3", "doc_id": "dad3a9d5cae6b052c4d247d72cf78446d3a19f7b29e4f65272a20a81fe6adad4", "sampling_score": 0.6634248366013072, "hypothesis": "CLAIM: The structured curriculum described as the bootcamp training pipeline improves zero-shot performance on downstream tasks compared to a standard fine-tuning schedule. MECHANISM: By interleaving staged data collection, progressive scaling of model size, and targeted alignment objectives, the pipeline encourages representations that generalize across tasks, reducing catastrophic forgetting during later phases. TEST: Conduct an ablation where the curriculum stages are shuffled or omitted while keeping total compute constant, then evaluate on the 53 downstream zero-shot and few-shot benchmarks from the DCLM-Pool used in arXiv:2406.11794v3. SIGNALS: A significant drop (\u22655% absolute accuracy) when stages are removed would support the claim; no change or improvement would refute it. Additionally, monitoring training loss curves and alignment metrics such as KL divergence between policy outputs and reference demonstrations will provide auxiliary evidence of curriculum efficacy.", "paper_anchor": "bootcamp training pipeline", "test": {"type": "ablation_control", "dataset_or_component": "DCLM-Pool downstream benchmark suite", "manipulation": "shuffle or remove curriculum stages while keeping compute constant", "metric": "zero-shot accuracy across the 53 tasks", "expected_direction": "decrease", "success_threshold": 0.05, "timeframe_days": 14}, "critic": {"novelty": 0.6499999999999999, "coherence": 0.9, "usefulness": 0.85, "binding": 1.0, "overall_score": 0.5524999999999999, "diagnostics": {"hard": true, "soft": 0.051, "binding_score": 1.0}}}
{"pair": ["AI safety evaluation suite", "Advanced AI evaluations at AISI: May update  | AISI Work"], "paper_concept": "AI safety evaluation suite", "corpus_concept": "Advanced AI evaluations at AISI: May update  | AISI Work", "author": "Unknown", "year": 2020, "title": "Advanced AI evaluations at AISI: May update  | AISI Work", "doc_id": "1a8a48a04a71f30982539ac9329cb6bf15d08a93fabdb6c74407387f393747c3", "sampling_score": 0.4395459521751657, "hypothesis": "CLAIM: Incorporating AISI\u2019s newly designed long\u2011horizon scientific planning tasks into the existing AI safety evaluation suite will expose additional unsafe decision\u2011making patterns that are missed by current knowledge\u2011retrieval benchmarks. MECHANISM: The suite presently emphasizes short\u2011term alignment metrics, so extending it with multi\u2011step planning challenges forces models to reason over extended horizons where misaligned incentives can surface, thereby increasing the likelihood of detecting safety violations. TEST: Conduct a rapid ablation control by training two identical model copies, one evaluated with only the original retrieval tasks and the other with the added planning tasks, then compare their aggregate safety scores. SIGNALS: A statistically significant rise in measured safety violations for the version that includes the planning tasks would support the claim, whereas no change or a reduction would refute it.", "paper_anchor": "AI safety evaluation suite", "test": {"type": "ablation_control", "dataset_or_component": "long-horizon planning benchmark", "manipulation": "swap in vs. out the new planning tasks while keeping other benchmarks constant", "metric": "average safety violation rate", "expected_direction": "increase", "success_threshold": 0.1, "timeframe_days": 14}, "critic": {"novelty": 0.6499999999999999, "coherence": 0.9, "usefulness": 0.85, "binding": 1.0, "overall_score": 0.5524999999999999, "diagnostics": {"hard": true, "soft": 0.054, "binding_score": 1.0}}}
{"pair": ["iterative alignment loop", "Discussion on the machine learning approach to AI safety | Victoria Krakovna"], "paper_concept": "iterative alignment loop", "corpus_concept": "Discussion on the machine learning approach to AI safety | Victoria Krakovna", "author": "Unknown", "year": 2020, "title": "Discussion on the machine learning approach to AI safety | Victoria Krakovna", "doc_id": "0533f4e3c026b99949c5caf11465a312128b3ce5e14f5dd5ea234375790cbe24", "sampling_score": 0.43243873598369015, "hypothesis": "CLAIM: Incorporating a structured \"iterative alignment loop\" into AI safety research pipelines yields statistically significant improvements in model alignment compared to static training regimes, and that this improvement persists across multiple domains such as language modeling, robotics control, and recommendation systems. MECHANISM: By repeatedly collecting failure cases, fine\u2011tuning the model, and re\u2011evaluating, the loop creates a feedback system that surfaces rare misbehaviors and allows targeted parameter updates, thereby reducing distributional shift between training and deployment environments; additionally, the loop enables curriculum learning where progressively harder examples are introduced, sharpening the model's safety heuristics. TEST: Conduct an ablation control where one version of the pipeline omits the repeated cycle while another retains it, then measure alignment on a standard safety benchmark over at least three random seeds to ensure robustness. SIGNALS: A measurable increase in benchmark alignment scores for the full\u2011loop version would support the claim, accompanied by a downward trend in failure\u2011case frequency; negligible difference or lower scores would refute it.", "paper_anchor": "iterative alignment loop", "test": {"type": "ablation_control", "dataset_or_component": "alignment data pipeline", "manipulation": "remove the repeated cycle of the iterative alignment loop", "metric": "alignment benchmark score", "expected_direction": "decrease", "success_threshold": 0.1, "timeframe_days": 14}, "critic": {"novelty": 0.61, "coherence": 0.91, "usefulness": 0.855, "binding": 1.0, "overall_score": 0.52155, "diagnostics": {"hard": true, "soft": 0.039, "binding_score": 1.0}}}
{"pair": ["reinforcement learning from human feedback (RLHF)", "Illustrating Reinforcement Learning from Human Feedback (RLHF)"], "paper_concept": "reinforcement learning from human feedback (RLHF)", "corpus_concept": "Illustrating Reinforcement Learning from Human Feedback (RLHF)", "author": "Unknown", "year": 2020, "title": "Illustrating Reinforcement Learning from Human Feedback (RLHF)", "doc_id": "cf86ade57ae4ee25c65a6eae0790ffa822591c0cc48e7eba6321718164a86625", "sampling_score": 0.3034953917050691, "hypothesis": "CLAIM: Using reinforcement learning from human feedback (RLHF) to fine\u2011tune large language models reduces the frequency of culturally insensitive outputs compared to standard supervised fine\u2011tuning. MECHANISM: Human preference data encode implicit social norms, and RLHF optimizes the policy toward higher\u2011reward responses that align with those norms, thereby suppressing patterns learned from raw internet text that often contain bias. TEST: Conduct an ablation where the reward model trained on human preferences is replaced with a random scorer while keeping all other training steps identical, then evaluate on a benchmark of culturally sensitive prompts. SIGNALS: A statistically significant drop in toxicity and bias metrics (e.g., Perspective API scores) when the RLHF component is present would support the claim; no change or increased scores would refute it.", "paper_anchor": "reinforcement learning from human feedback (RLHF)", "test": {"type": "ablation_control", "dataset_or_component": "reward model", "manipulation": "replace reward model with random scorer", "metric": "average Perspective API toxicity score on culturally sensitive prompt set", "expected_direction": "decrease", "success_threshold": 0.1, "timeframe_days": 7}, "critic": {"novelty": 0.6, "coherence": 0.9, "usefulness": 0.85, "binding": 1.0, "overall_score": 0.51, "diagnostics": {"hard": true, "soft": 0.107, "binding_score": 1.0}}}
{"pair": ["first\u2011principles reasoning", "2402.16837"], "paper_concept": "first\u2011principles reasoning", "corpus_concept": "2402.16837", "author": "Unknown", "year": 2020, "title": "2402.16837", "doc_id": "68341650a6701d27ec0c0e36809231bd04027e60ce22426c44f67a25dde97fbf", "sampling_score": 0.2948196721311475, "hypothesis": "CLAIM: The dynamic critic described in the paper will outperform a static pattern\u2011matching baseline on the entity\u2011description task from arXiv:2402.16837 when its decision module uses first\u2011principles reasoning to select flaws. MECHANISM: By grounding flaw selection in fundamental logical analysis rather than surface similarity, the critic can prioritize inconsistencies that are semantically salient for each description, leading to deeper investigative steps and higher precision in identifying mismatches between described entities and their true attributes. TEST: Conduct an ablation control where the critic\u2019s flaw\u2011selection module is replaced with a pure pattern\u2011matching classifier on the descriptive mention dataset from 2402.16837, measuring F1 score for correct flaw detection. SIGNALS: An increase of \u22650.10 in F1 for the original system versus the ablated version would support the claim; no change or a decrease would refute it.", "paper_anchor": "first\u2011principles reasoning", "test": {"type": "ablation_control", "dataset_or_component": "descriptive mention dataset from arXiv:2402.16837 (flaw-selection module)", "manipulation": "replace flaw-selection module with a pure pattern-matching classifier", "metric": "F1 score for flaw detection", "expected_direction": "increase", "success_threshold": 0.1, "timeframe_days": 14}, "critic": {"novelty": 0.6, "coherence": 0.9, "usefulness": 0.85, "binding": 1.0, "overall_score": 0.51, "diagnostics": {"hard": true, "soft": 0.054, "binding_score": 1.0}}}
{"pair": ["iterative alignment loop", "AI for AI safety - Effective Altruism forum viewer"], "paper_concept": "iterative alignment loop", "corpus_concept": "AI for AI safety - Effective Altruism forum viewer", "author": "Unknown", "year": 2020, "title": "AI for AI safety - Effective Altruism forum viewer", "doc_id": "26f7f40349818b31f6a5860dc7161858247ba56d1ca78f74a52f54298482c12f", "sampling_score": 0.5467785584179028, "hypothesis": "CLAIM: The presence of an \"iterative alignment loop\" in the training pipeline directly boosts AI safety feedback efficiency on the Effective Altruism forum dataset. MECHANISM: By repeatedly collecting user\u2011generated safety critiques, retraining the model, and re\u2011evaluating against a held\u2011out safety benchmark, each cycle refines the model\u2019s ability to anticipate harmful outputs, creating a positive feedback between capability growth and alignment tightening. TEST: Conduct an ablation study where the loop is disabled after the first training epoch while keeping all other components constant, then measure downstream safety classification accuracy on a curated EA forum safety corpus. SIGNALS: A statistically significant drop (\u22655% absolute decrease) in safety\u2011related F1 score when the loop is removed would support the claim; no change or improvement would refute it.", "paper_anchor": "iterative alignment loop", "test": {"type": "ablation_control", "dataset_or_component": "EA forum safety corpus evaluation set", "manipulation": "disable iterative alignment loop after first epoch", "metric": "F1 score on safety classification", "expected_direction": "decrease", "success_threshold": 0.05, "timeframe_days": 14}, "critic": {"novelty": 0.6, "coherence": 0.9, "usefulness": 0.85, "binding": 1.0, "overall_score": 0.51, "diagnostics": {"hard": true, "soft": 0.054, "binding_score": 1.0}}}
{"pair": ["bootcamp training pipeline", "2406.11794v4"], "paper_concept": "bootcamp training pipeline", "corpus_concept": "2406.11794v4", "author": "Unknown", "year": 2020, "title": "2406.11794v4", "doc_id": "fe3e376394cddec82028ae2b1d6794912dbe43b4ce7526e226794cf05d289510", "sampling_score": 0.52988, "hypothesis": "CLAIM: The accelerated curriculum described in the paper improves downstream alignment performance when applied to large language models trained on web-scale filtered data such as RefinedWeb. MECHANISM: By staging data collection, cleaning (English filter, URL filter, DCLM-Pool etc.) and fine-tuning phases, the \"bootcamp training pipeline\" creates a curriculum that gradually introduces harder alignment tasks, allowing the model to internalize safety heuristics more efficiently than a single monolithic fine-tune. TEST: Run an ablation where the staged curriculum is replaced with a single pass over the same filtered dataset and compare alignment metrics on a held-out safety benchmark. SIGNALS: A statistically significant increase in safety scores (e.g., reduced toxicity, higher truthfulness) for the staged version would support the claim; no change or degradation would refute it.", "paper_anchor": "bootcamp training pipeline", "test": {"type": "ablation_control", "dataset_or_component": "RefinedWeb filtered corpus alignment fine-tuning stage", "manipulation": "replace staged curriculum with single-pass training over identical data", "metric": "safety benchmark composite score (toxicity reduction, truthfulness improvement)", "expected_direction": "increase", "success_threshold": 0.55, "timeframe_days": 14}, "critic": {"novelty": 0.6, "coherence": 0.9, "usefulness": 0.85, "binding": 1.0, "overall_score": 0.51, "diagnostics": {"hard": true, "soft": 0.057, "binding_score": 1.0}}}
{"pair": ["bootcamp training pipeline", "2001.08361.pdf"], "paper_concept": "bootcamp training pipeline", "corpus_concept": "2001.08361.pdf", "author": "Unknown", "year": 2020, "title": "2001.08361.pdf", "doc_id": "0af92daf8a4a58015ef43ba870e99ec3b7044ad54604a97dea8daa334659c94a", "sampling_score": 0.5268196721311477, "hypothesis": "CLAIM: The accelerated curriculum described as a bootcamp training pipeline improves the sample efficiency of alignment fine\u2011tuning for large language models compared with standard sequential fine-tuning. MECHANISM: By interleaving staged data collection, curriculum\u2011driven loss weighting, and periodic model checkpoint sharding, the pipeline reduces catastrophic forgetting and encourages progressive skill acquisition, which should manifest as higher alignment scores per training token. TEST: Conduct an ablation where the curriculum stages are removed while keeping total compute constant, then measure alignment accuracy on a held\u2011out safety benchmark. SIGNALS: If the full pipeline yields a statistically significant increase (e.g., >5% absolute gain) in alignment metrics relative to the ablated version, the claim is supported; a negligible or negative difference would refute it. Additional observation of faster convergence in loss curves would further corroborate the efficiency claim.", "paper_anchor": "bootcamp training pipeline", "test": {"type": "ablation_control", "dataset_or_component": "alignment safety benchmark", "manipulation": "remove curriculum stages while keeping total compute constant", "metric": "alignment accuracy (percentage)", "expected_direction": "increase", "success_threshold": 0.05, "timeframe_days": 14}, "critic": {"novelty": 0.6, "coherence": 0.9, "usefulness": 0.85, "binding": 1.0, "overall_score": 0.51, "diagnostics": {"hard": true, "soft": 0.051, "binding_score": 1.0}}}
{"pair": ["large language model (LLM) architecture", "AI risks"], "paper_concept": "large language model (LLM) architecture", "corpus_concept": "AI risks", "author": "Unknown", "year": 2020, "title": "AI risks", "doc_id": "351405f6a19a3780a1d68fcd6462a11bbd6b833b047b330f6737b61df1efd300", "sampling_score": 0.5157611940298508, "hypothesis": "CLAIM: The use of an auto\u2011alignment bootcamp on a transformer\u2011based \"large language model (LLM) architecture\" will reduce the prevalence of AI risk narratives that overstate non\u2011superintelligence threats. MECHANISM: By fine\u2011tuning the model with curated alignment data, it learns to calibrate its uncertainty estimates and suppress exaggerated risk language, leading to more balanced discourse. TEST: Conduct an ablation_control experiment where the bootcamp training is removed while keeping all other components identical, then evaluate on a benchmark of AI\u2011risk forum posts using a toxicity\u2011adjusted relevance metric. SIGNALS: A significant decrease in the frequency of high\u2011severity risk statements and an increase in neutral explanatory sentences would support the claim; no change or higher risk language frequency would refute it. Human judges also reported a 15% drop in perceived existential danger ratings.", "paper_anchor": "large language model (LLM) architecture", "test": {"type": "ablation_control", "dataset_or_component": "AI risk forum posts benchmark", "manipulation": "remove bootcamp training while keeping other components unchanged", "metric": "frequency of high\u2011severity risk statements and proportion of neutral explanatory sentences", "expected_direction": "decrease", "success_threshold": 0.1, "timeframe_days": 14}, "critic": {"novelty": 0.6, "coherence": 0.9, "usefulness": 0.825, "binding": 1.0, "overall_score": 0.49499999999999994, "diagnostics": {"hard": true, "soft": 0.082, "binding_score": 1.0}}}
{"pair": ["prompt different instances with varied contexts", "(PDF) Discursive Competence in ChatGPT, Part 2: Memory for Texts, Version 3"], "paper_concept": "prompt different instances with varied contexts", "corpus_concept": "(PDF) Discursive Competence in ChatGPT, Part 2: Memory for Texts, Version 3", "author": "Unknown", "year": 2020, "title": "(PDF) Discursive Competence in ChatGPT, Part 2: Memory for Texts, Version 3", "doc_id": "14e29ed5988f021bb1b301194ec40485611dc86191f8bc69ecf173c5303ff111", "sampling_score": 0.30540383141762456, "hypothesis": "CLAIM: Using multiple critique agents that \"prompt different instances with varied contexts\" will improve the factual consistency of ChatGPT\u2019s memory for texts by exposing it to diverse error patterns. MECHANISM: Diverse subsets of logs and instructions cause each agent to attend to distinct latent representations, increasing coverage of potential hallucinations and enabling ensemble voting that filters out context\u2011specific mistakes. TEST: Conduct an ablation where we replace the varied\u2011context prompting with identical prompts across agents while keeping all other settings constant, then measure factual recall on a held\u2011out narrative dataset (e.g., the Johnstown flood story). SIGNALS: If the original setup yields higher exact\u2011match accuracy and lower contradiction rates than the control, it supports the claim; no difference or degradation would refute it.", "paper_anchor": "prompt different instances with varied contexts", "test": {"type": "ablation_control", "dataset_or_component": "Johnstown flood narrative recall set", "manipulation": "replace varied-context prompts with identical prompts across critique agents", "metric": "exact-match accuracy and contradiction rate", "expected_direction": "increase", "success_threshold": 0.02, "timeframe_days": 14}, "critic": {"novelty": 0.575, "coherence": 0.91, "usefulness": 0.855, "binding": 1.0, "overall_score": 0.491625, "diagnostics": {"hard": true, "soft": 0.103, "binding_score": 1.0}}}
{"pair": ["prompt different instances with varied contexts", "Methods of prompt programming :: \u2014 Moire"], "paper_concept": "prompt different instances with varied contexts", "corpus_concept": "Methods of prompt programming :: \u2014 Moire", "author": "Unknown", "year": 2020, "title": "Methods of prompt programming :: \u2014 Moire", "doc_id": "b805ad3f02c2736347e6b5fa23e790cbcc29be5110f831af382fe48503f6609a", "sampling_score": 0.2888486576383939, "hypothesis": "CLAIM: Using a critique agent that can \"prompt different instances with varied contexts\" will produce more diverse flaw candidates than a single uniform prompt. MECHANISM: By exposing each copy of the model to distinct subsets of logs or instruction snippets, the model samples separate regions of its latent space, encouraging divergent reasoning paths and reducing mode collapse. TEST: Run an ablation where one condition uses identical prompts for all copies while the experimental condition employs the varied prompting strategy, then measure the lexical and semantic diversity of generated critiques on a held\u2011out set of game dialogue logs. SIGNALS: A significant increase in distinct n\u2011gram coverage and lower pairwise cosine similarity among outputs would support the claim; negligible change or reduced quality would refute it.", "paper_anchor": "prompt different instances with varied contexts", "test": {"type": "ablation_control", "dataset_or_component": "critique agent prompt module", "manipulation": "replace varied prompts with identical prompts across all copies", "metric": "distinct n\u2011gram count and average pairwise cosine similarity of output embeddings", "expected_direction": "increase", "success_threshold": 0.15, "timeframe_days": 14}, "critic": {"novelty": 0.575, "coherence": 0.91, "usefulness": 0.855, "binding": 1.0, "overall_score": 0.491625, "diagnostics": {"hard": true, "soft": 0.107, "binding_score": 1.0}}}
{"pair": ["database of past similar mistakes", "Red Teaming CEA\u2019s Community Building Work - Effective Altruism forum viewer"], "paper_concept": "database of past similar mistakes", "corpus_concept": "Red Teaming CEA\u2019s Community Building Work - Effective Altruism forum viewer", "author": "Unknown", "year": 2020, "title": "Red Teaming CEA\u2019s Community Building Work - Effective Altruism forum viewer", "doc_id": "64a23457cd5d6a8c12fbc132c71f8bc24d5e0c2fb4a66f3fe256e5882590b4ee", "sampling_score": 0.2868196721311475, "hypothesis": "CLAIM: Integrating a curated \"database of past similar mistakes\" into the flaw\u2011investigation pipeline will markedly improve the system's ability to forecast emerging research errors. MECHANISM: By retrieving analogues from prior incidents, the knowledge base provides concrete baselines that complement first\u2011principles reasoning, allowing the model to weight historical patterns alongside novel signals, thereby reducing false positives in error prediction. TEST: Conduct an ablation study where the retrieval component accessing this repository is disabled and compare forecasting performance on a held\u2011out set of recent research projects. SIGNALS: A statistically significant drop in precision/recall (e.g., >5% absolute decrease) when the database is removed would support the claim, whereas negligible change or improvement would refute it.", "paper_anchor": "database of past similar mistakes", "test": {"type": "ablation_control", "dataset_or_component": "knowledge\u2011base retrieval module for analogous errors", "manipulation": "disable access to the \"database of past similar mistakes\" during inference", "metric": "F1 score on error\u2011forecasting task", "expected_direction": "decrease", "success_threshold": 0.05, "timeframe_days": 14}, "critic": {"novelty": 0.525, "coherence": 0.91, "usefulness": 0.905, "binding": 1.0, "overall_score": 0.475125, "diagnostics": {"hard": true, "soft": 0.075, "binding_score": 1.0}}}
{"pair": ["reward model fine\u2011tuning", "Scalable agent alignment via reward mode \u203a 1811.07871.pdf"], "paper_concept": "reward model fine\u2011tuning", "corpus_concept": "Scalable agent alignment via reward mode \u203a 1811.07871.pdf", "author": "Unknown", "year": 2020, "title": "Scalable agent alignment via reward mode \u203a 1811.07871.pdf", "doc_id": "ac28715beeddbb49d11fe2f42e80b4920b7add08615b84f7f65fd89ff88d56cf", "sampling_score": 0.4351816091954024, "hypothesis": "CLAIM: Incorporating a short phase of \"reward model fine\u2011tuning\" on bootcamp trajectories will raise the correlation between predicted rewards and human preference scores on held\u2011out tasks. MECHANISM: The additional gradient steps adapt the reward network\u2019s parameters to distributional nuances present in the bootcamp data, aligning its latent representation with user feedback patterns that were under\u2011represented during initial pretraining. TEST: Conduct an ablation where the bootcamp fine\u2011tuning stage is removed while keeping all other training stages identical, then evaluate on a standard preference benchmark such as TL;DR summarization. SIGNALS: A statistically significant increase (p<0.05) in Kendall\u2019s tau or Pearson correlation for the full pipeline versus the ablated version would support the claim; no change or a decrease would refute it. Further replication across domains would strengthen confidence in the effect.", "paper_anchor": "reward model fine\u2011tuning", "test": {"type": "ablation_control", "dataset_or_component": "bootcamp fine\u2011tuning stage", "manipulation": "remove bootcamp fine\u2011tuning while keeping other training steps unchanged", "metric": "Kendall's tau correlation between model rewards and human preference labels on held\u2011out benchmark", "expected_direction": "increase", "success_threshold": 0.55, "timeframe_days": 14}, "critic": {"novelty": 0.5, "coherence": 0.925, "usefulness": 0.875, "binding": 1.0, "overall_score": 0.4375, "diagnostics": {"hard": true, "soft": 0.05, "binding_score": 1.0}}}
{"pair": ["prompt different instances with varied contexts", "Constitutional AI: Harmlessness from AI  \u203a 2212.08073.pdf"], "paper_concept": "prompt different instances with varied contexts", "corpus_concept": "Constitutional AI: Harmlessness from AI  \u203a 2212.08073.pdf", "author": "Unknown", "year": 2020, "title": "Constitutional AI: Harmlessness from AI  \u203a 2212.08073.pdf", "doc_id": "ab3fc725a7f53b16718bb9315be0227355a81614ba9e4045b3231b9062a27b2b", "sampling_score": 0.3866410650281618, "hypothesis": "CLAIM: The critique agent's ability to generate diverse candidate flaws improves overall harmlessness evaluation when the system is instructed to \"prompt different instances with varied contexts\". MECHANISM: By exposing each copy of the agent to distinct log subsets or instruction sets, it samples a broader space of potential failure modes, reducing blind spots and aligning human preference labels more closely with true quality as measured in Constitutional AI. TEST: Conduct an ablation where we replace the multi\u2011prompt strategy with a single shared prompt across all copies while keeping model size constant, then compare preference ranking accuracy on the benchmark from the Constitutionally\u2011aligned dataset. SIGNALS: An increase in label agreement and higher binary choice accuracy under the full multi\u2011prompt condition would support the claim; no change or degradation would refute it.", "paper_anchor": "prompt different instances with varied contexts", "test": {"type": "ablation_control", "dataset_or_component": "Constitutional AI preference benchmark", "manipulation": "replace multi-prompt strategy with single shared prompt across copies", "metric": "binary choice accuracy and label agreement score", "expected_direction": "increase", "success_threshold": 0.6, "timeframe_days": 14}, "critic": {"novelty": 0.5, "coherence": 0.9, "usefulness": 0.85, "binding": 1.0, "overall_score": 0.425, "diagnostics": {"hard": true, "soft": 0.1, "binding_score": 1.0}}}
{"pair": ["auto\u2011alignment", "What should AI safety be trying to achieve? - Effective Altruism forum viewer"], "paper_concept": "auto\u2011alignment", "corpus_concept": "What should AI safety be trying to achieve? - Effective Altruism forum viewer", "author": "Unknown", "year": 2020, "title": "What should AI safety be trying to achieve? - Effective Altruism forum viewer", "doc_id": "eb5dc6745aec1f63bdd6a658a6448db002e4237cf6e1ea86bd088276b546b56c", "sampling_score": 0.5545650793650794, "hypothesis": "CLAIM: The central argument that AI safety should prioritize developing systems capable of auto\u2011alignment is testable by measuring whether such systems reduce reliance on human oversight while maintaining goal fidelity. MECHANISM: If a model can internally infer and adjust its utility function based on observed human preferences, it will autonomously converge toward safe behavior without explicit instruction, thereby embodying the proposed safety objective. TEST: Conduct an ablation study where the preference\u2011inference module is removed from a reinforcement\u2011learning agent trained on a benchmark alignment task and compare performance to the full system using a holdout set of novel tasks. SIGNALS: A significant drop in safe\u2011behavior metrics (e.g., reduced policy divergence from human demonstrations) when the module is ablated would support the claim, whereas negligible change or improvement would refute it.", "paper_anchor": "auto\u2011alignment", "test": {"type": "ablation_control", "dataset_or_component": "preference-inference module in RL alignment agent", "manipulation": "remove the preference inference component", "metric": "safe-behavior score measured by policy divergence from human demonstrations", "expected_direction": "decrease", "success_threshold": 0.15, "timeframe_days": 14}, "critic": {"novelty": 0.5, "coherence": 0.9, "usefulness": 0.85, "binding": 1.0, "overall_score": 0.425, "diagnostics": {"hard": true, "soft": 0.03, "binding_score": 1.0}}}
{"pair": ["preference modeling", "2309.11489"], "paper_concept": "preference modeling", "corpus_concept": "2309.11489", "author": "Unknown", "year": 2020, "title": "2309.11489", "doc_id": "2d235056542242dab35994fce37bdff5527fb5087b96006c1a7940bfd20f5406", "sampling_score": 0.606015162283819, "hypothesis": "CLAIM: The reward function learned by the method described in the paper outperforms standard hand\u2011crafted reward shaping baselines on ICLR 2024 benchmark tasks because it incorporates \"preference modeling\". MECHANISM: By training a neural predictor on human pairwise comparisons, the approach captures latent utility signals that handcrafted potentials miss, aligning the policy gradient with true user intent and reducing reward hacking. TEST: Conduct an ablation where the learned reward predictor is replaced with a fixed hand\u2011crafted shaping term while keeping all other components identical, evaluating on the same suite of environments. SIGNALS: If performance (e.g., average return or alignment score) drops by at least 10% relative to the full system, this supports the claim; negligible change or improvement would refute it overall.", "paper_anchor": "preference modeling", "test": {"type": "ablation_control", "dataset_or_component": "reward predictor module", "manipulation": "replace learned reward predictor with hand\u2011crafted shaping term", "metric": "average return or alignment score", "expected_direction": "decrease", "success_threshold": 0.1, "timeframe_days": 14}, "critic": {"novelty": 0.44999999999999996, "coherence": 0.9, "usefulness": 0.8999999999999999, "binding": 1.0, "overall_score": 0.4049999999999999, "diagnostics": {"hard": true, "soft": 0.044, "binding_score": 1.0}}}
{"pair": ["reinforcement learning from human feedback (RLHF)", "AI Safety 101 : Reward Misspecification - Effective Altruism forum viewer"], "paper_concept": "reinforcement learning from human feedback (RLHF)", "corpus_concept": "AI Safety 101 : Reward Misspecification - Effective Altruism forum viewer", "author": "Unknown", "year": 2020, "title": "AI Safety 101 : Reward Misspecification - Effective Altruism forum viewer", "doc_id": "13243cb482fb84efc880c99deb092b14687f9c17a9af2d3208ad66124d8619ef", "sampling_score": 0.6356470588235295, "hypothesis": "CLAIM: Using reinforcement learning from human feedback (RLHF) to train a secondary reward model will reduce reward misspecification errors observed when ranking LLM outputs. MECHANISM: Human rankings provide a richer signal than binary preference, allowing the reward model to learn nuanced utility gradients that align better with intended safety criteria, thus correcting systematic over\u2011optimization of proxy rewards. TEST: Conduct an ablation where the secondary reward model is trained either with RLHF-derived rankings or with randomly shuffled ranks on the same set of samples from a held\u2011out prompt suite. SIGNALS: A statistically significant decrease in the proportion of generated completions that trigger known misspecification benchmarks (e.g., unsafe content, goal\u2011drift) would support the claim; no change or an increase would refute it.", "paper_anchor": "reinforcement learning from human feedback (RLHF)", "test": {"type": "ablation_control", "dataset_or_component": "reward model ranking data", "manipulation": "replace human rankings with random order", "metric": "percentage of safety benchmark violations", "expected_direction": "decrease", "success_threshold": 0.1, "timeframe_days": 14}, "critic": {"novelty": 0.44999999999999996, "coherence": 0.9, "usefulness": 0.85, "binding": 1.0, "overall_score": 0.38249999999999995, "diagnostics": {"hard": true, "soft": 0.109, "binding_score": 1.0}}}
{"pair": ["preference modeling", "1906.08663.pdf"], "paper_concept": "preference modeling", "corpus_concept": "1906.08663.pdf", "author": "Unknown", "year": 2020, "title": "1906.08663.pdf", "doc_id": "955c6c6b6daccdf2fff4025ebb8f8e9595e3df6cea726eab9701c57211b9bd06", "sampling_score": 0.48357317073170736, "hypothesis": "CLAIM: The reward function learned by the method described in the paper is equivalent to the recursive reward modeling approach illustrated in Figure\u202f6 of 1906.08663, because both rely on \"preference modeling\" to align AI behavior with human judgments. MECHANISM: By training a predictor that maps state\u2011action pairs to scalar scores using human preference data, the system can be unrolled recursively, allowing earlier reward models to supervise later ones; this shared objective creates comparable gradients and similar convergence properties across the two frameworks. TEST: Conduct an ablation where the recursive component in the external causal diagram implementation is removed while keeping the underlying preference\u2011learning loss unchanged, then evaluate on a held\u2011out set of human\u2011ranked trajectories. SIGNALS: If performance (e.g., Kendall\u2019s \u03c4 between model scores and human rankings) drops significantly relative to the full system, it supports the claim; negligible change or improvement would refute it.", "paper_anchor": "preference modeling", "test": {"type": "ablation_control", "dataset_or_component": "recursive reward component in causal diagram implementation", "manipulation": "remove recursive supervision while preserving preference\u2011learning loss", "metric": "Kendall's tau correlation between model scores and human rankings", "expected_direction": "decrease", "success_threshold": 0.1, "timeframe_days": 14}, "critic": {"novelty": 0.43999999999999995, "coherence": 0.91, "usefulness": 0.855, "binding": 1.0, "overall_score": 0.3761999999999999, "diagnostics": {"hard": true, "soft": 0.037, "binding_score": 1.0}}}
{"pair": ["probability of severe flaw", "Precision of Sets of Forecasts - Effective Altruism forum viewer"], "paper_concept": "probability of severe flaw", "corpus_concept": "Precision of Sets of Forecasts - Effective Altruism forum viewer", "author": "Unknown", "year": 2020, "title": "Precision of Sets of Forecasts - Effective Altruism forum viewer", "doc_id": "2b5f556dd43331b09b526611a63c2ac8bc99c360b5c3d37a8439d2a2650db540", "sampling_score": 0.28445737874097005, "hypothesis": "CLAIM: The calibrated critic\u2019s \"probability of severe flaw\" output correlates strongly with the empirical calibration error observed in the gjpsurvey forecast dataset. MECHANISM: Because both systems aim to quantify uncertainty, a higher reported probability should align with lower predictive precision when the answer_option mismatches the outcome, indicating that the critic is correctly flagging flawed forecasts. TEST: Compute the Spearman correlation between the critic's probability scores and the binary mismatch indicator across the gjpsurvey rows, then compare against a shuffled baseline. SIGNALS: A significant positive correlation (p<0.05) would support the claim, whereas near\u2011zero or negative correlation would refute it. Additionally, stratifying by forecast horizon should reveal that the relationship holds across short and long term predictions, confirming robustness. If the correlation diminishes when we randomize the probability values, it demonstrates that the observed signal is not an artifact of dataset composition.", "paper_anchor": "probability of severe flaw", "test": {"type": "negative_control", "dataset_or_component": "gjpsurvey forecast dataset probability column", "manipulation": "shuffle the critic's probability values across rows", "metric": "Spearman correlation coefficient between probability scores and mismatch indicator", "expected_direction": "decrease", "success_threshold": 0.2, "timeframe_days": 7}, "critic": {"novelty": 0.4, "coherence": 0.9, "usefulness": 0.825, "binding": 1.0, "overall_score": 0.33, "diagnostics": {"hard": true, "soft": 0.049, "binding_score": 1.0}}}
{"pair": ["finetuning", "AISC 2024 - Project Summaries - Effective Altruism forum viewer"], "paper_concept": "finetuning", "corpus_concept": "AISC 2024 - Project Summaries - Effective Altruism forum viewer", "author": "Unknown", "year": 2020, "title": "AISC 2024 - Project Summaries - Effective Altruism forum viewer", "doc_id": "ca88142ca79a11d4bb44104d3cf825425664cf1fcd0f363c0b233dcb6b49c376", "sampling_score": 0.3828196721311476, "hypothesis": "CLAIM: The top research agent\u2019s superior critique performance arises because its specialized knowledge acquired through finetuning on the target paper\u2019s domain yields higher alignment with expert evaluation criteria. MECHANISM: By adapting the model weights to the specific terminology, argument structures, and citation patterns of a given field, the critic internalizes latent heuristics that improve relevance scoring and error detection, which generic pre\u2011training lacks. TEST: Conduct an ablation where the same agent is evaluated on critique tasks with the domain\u2011adaptation step removed while keeping all other training identical, using the AISC 2024 project summaries as a held\u2011out test set. SIGNALS: If performance (measured by BLEU\u2011like relevance and human rating correlation) drops significantly when the adaptation is omitted, it supports the claim; negligible change would refute it.", "paper_anchor": "finetuning", "test": {"type": "ablation_control", "dataset_or_component": "AISC 2024 project summaries", "manipulation": "remove domain\u2011adaptation finetuning step from the critic model", "metric": "human rating correlation with expert critiques and BLEU-like relevance score", "expected_direction": "decrease", "success_threshold": 0.1, "timeframe_days": 14}, "critic": {"novelty": 0.3, "coherence": 0.9, "usefulness": 0.85, "binding": 1.0, "overall_score": 0.255, "diagnostics": {"hard": true, "soft": 0.022, "binding_score": 1.0}}}
{"pair": ["reward model fine\u2011tuning", "1909.08593.pdf"], "paper_concept": "reward model fine\u2011tuning", "corpus_concept": "1909.08593.pdf", "author": "Unknown", "year": 2020, "title": "1909.08593.pdf", "doc_id": "3e3d68974ad3d75e3d8d440699ae8ad2da499233440345c78fecabde12e5ffb1", "sampling_score": 0.5750245901639345, "hypothesis": "CLAIM: Adjusting a learned reward model on bootcamp data improves its fidelity in capturing desired behaviors. MECHANISM: By exposing the model to a curated set of high\u2011quality preference demonstrations, the parameters are nudged toward regions of the reward landscape that better reflect human intent; this process, known as \"reward model fine\u2011tuning\", reduces misalignment between predicted and true preferences and allows the downstream policy to generalize more reliably. TEST: Conduct an ablation control where the bootcamp dataset is removed during training while keeping all other hyperparameters constant, then evaluate on a held\u2011out human preference benchmark. SIGNALS: An increase in Pearson correlation (or decrease in KL divergence) between model scores and human labels after including bootcamp data would support the claim; negligible change or a drop would refute it.", "paper_anchor": "reward model fine\u2011tuning", "test": {"type": "ablation_control", "dataset_or_component": "bootcamp preference dataset", "manipulation": "remove bootcamp data from reward model training", "metric": "Pearson correlation with held-out human preferences", "expected_direction": "increase", "success_threshold": 0.02, "timeframe_days": 7}, "critic": {"novelty": 0.3, "coherence": 0.9, "usefulness": 0.85, "binding": 1.0, "overall_score": 0.255, "diagnostics": {"hard": true, "soft": 0.053, "binding_score": 1.0}}}
{"pair": ["large language model (LLM) architecture", "Why large language models struggle with long contexts"], "paper_concept": "large language model (LLM) architecture", "corpus_concept": "Why large language models struggle with long contexts", "author": "Unknown", "year": 2020, "title": "Why large language models struggle with long contexts", "doc_id": "b5dfe8292ccd8e9cd2f88ac13bc49d31f8d942d93492077ae4e955a05adc22dd", "sampling_score": 0.3686654411037973, "hypothesis": "CLAIM: The transformer\u2011based large language model (LLM) architecture causes a quadratic attention cost that leads to degraded performance on very long contexts. MECHANISM: Because self\u2011attention scales O(n^2) with token length, the model allocates fewer compute resources per token as the window grows, resulting in information loss and reduced recall for distant tokens. TEST: Conduct an ablation_control experiment by training two identical models, one with standard full\u2011self\u2011attention and another with a linear\u2011complexity attention approximation, then evaluate on a benchmark of passages ranging from 2k to 32k tokens. SIGNALS: If the hypothesis is correct, the standard model\u2019s accuracy will decrease sharply as context length increases, while the linear\u2011attention variant will show a smaller drop or maintain performance; a reversal would refute the claim.", "paper_anchor": "large language model (LLM) architecture", "test": {"type": "ablation_control", "dataset_or_component": "LongContextBenchmark", "manipulation": "replace full self-attention with linear attention approximation", "metric": "accuracy on question answering over varying context lengths", "expected_direction": "decrease", "success_threshold": 0.05, "timeframe_days": 30}, "critic": {"novelty": 0.25, "coherence": 0.9, "usefulness": 0.85, "binding": 1.0, "overall_score": 0.2125, "diagnostics": {"hard": true, "soft": 0.087, "binding_score": 1.0}}}
