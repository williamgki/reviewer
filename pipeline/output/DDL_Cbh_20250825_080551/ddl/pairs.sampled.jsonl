{"paper_concept": "Phase Transitions in Model Training", "corpus_concept": "How might we align transformative AI if it\u2019s developed very soon? - Effective Altruism forum viewer", "paper_backpack": "The concept of dividing a model's learning trajectory into distinct regimes (stable phases and rapid transition zones) to identify predictable vs. malleable behavior for interpretability and control.", "corpus_snippet": ".) So an AI system that is seeking to maximize reward (or\n\nbehave well in training\n\nseek to disempower humans when it later has a chance to do so.\n\nMagma\u2019s goal is to achieve extremely high reliabilit", "author": "Unknown", "year": 2020, "title": "How might we align transformative AI if it\u2019s developed very soon? - Effective Altruism forum viewer", "doc_id": "b37d2c737bda30727494cc60ba6538552115fcfcff2db1baf9b5d6ff77d9e8f6", "novelty": 1.0, "plausibility": 0.22406151062867483, "diversity": 1.0, "score": 0.47563998190863865, "paper_anchor_exact": "Phase Transitions in Model Training", "paper_anchor_alias": "Phase Transitions in Model Training"}
{"paper_concept": "Transformers", "corpus_concept": "Transformers are Graph Neural Networks | NTU Graph Deep Learning Lab", "paper_backpack": "The dominant neural architecture for large language models; its self\u2011attention layers exhibit the discussed phase\u2011transition signatures such as abrupt in\u2011context learning and grokking.. Context from paper: Remarkably, transformers display parallel signatures: in-context learning appears abruptly once context length crosses a threshold, single fine-tune steps or jailbreak prompts can flip global behaviour, and \u201cgrokking\u201d produces a critical slowing-down familiar from continuous phase transitions", "corpus_snippet": "Transformers are Graph Neural Networks\n\nEngineer friends often ask me: Graph Deep Learning sounds great, but are there any big commercial success stories? Is it being deployed in practical application", "author": "Unknown", "year": 2020, "title": "Transformers are Graph Neural Networks | NTU Graph Deep Learning Lab", "doc_id": "96ff50852ac2d825b088562b3dac9b3f9d6ef12c128605601e9b6c6bd6d07ce9", "novelty": 0.9, "plausibility": 0.22462121212121214, "diversity": 1.0, "score": 0.4560037878787879, "paper_anchor_exact": "Transformers", "paper_anchor_alias": "Transformers"}
{"paper_concept": "Singular Learning Theory (SLT)", "corpus_concept": "Singular Learning Theory seminar | metauni", "paper_backpack": "A theoretical framework from statistical learning that models how large neural networks undergo abrupt qualitative changes\u2014treated as phase transitions\u2014to explain emergent behaviors during training.. Context from paper: Phase Transitions\nDevelopmental interpretability is this really exciting research agenda that proposes we use singular learning theory (SLT) to study the development of language models in terms of phase transitions", "corpus_snippet": "The new SLT seminar can be found\n\nThis is the homepage of a seminar on Singular Learning Theory (SLT), a theory applying algebraic geometry to statistical learning theory founded by\n\n. The seminar tak", "author": "Unknown", "year": 2020, "title": "Singular Learning Theory seminar | metauni", "doc_id": "8ce21bced6af2eb2dddd21960b3d516670167ca0ad295ad09ae72410b60d48a3", "novelty": 0.5714285714285714, "plausibility": 0.22622950819672133, "diversity": 1.0, "score": 0.35847775175644025, "paper_anchor_exact": "Singular Learning Theory (SLT)", "paper_anchor_alias": "Singular Learning Theory (SLT)"}
